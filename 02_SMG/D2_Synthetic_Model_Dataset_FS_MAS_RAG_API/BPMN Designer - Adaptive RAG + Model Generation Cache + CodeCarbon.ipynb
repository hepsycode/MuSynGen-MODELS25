{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df98739-ce21-4ab1-9b58-f648b5ffa1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82548e7c-2e03-4806-9ca4-b93b268a183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRICE_PER_INPUT_TOKEN: 1e-07 $\n",
      "PRICE_PER_OUTPUT_TOKEN: 3e-07 $\n",
      "Model Name: mistral-small-latest\n",
      "Model Temperature: 0.0\n",
      "Loaded 9 URLs from 'config/BASE_URL.csv'.\n",
      "The folder 'faiss' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001883C5F35B0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vitto\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 790, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "  File \"C:\\Users\\vitto\\anaconda3\\lib\\threading.py\", line 1396, in enumerate\n",
      "    with _active_limbo_lock:\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001883C5F35B0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vitto\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 790, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "  File \"C:\\Users\\vitto\\anaconda3\\lib\\threading.py\", line 1396, in enumerate\n",
      "    with _active_limbo_lock:\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001883C5F35B0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vitto\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 790, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "  File \"C:\\Users\\vitto\\anaconda3\\lib\\threading.py\", line 1396, in enumerate\n",
      "    with _active_limbo_lock:\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001883C5F35B0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vitto\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 790, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "  File \"C:\\Users\\vitto\\anaconda3\\lib\\threading.py\", line 1396, in enumerate\n",
      "    with _active_limbo_lock:\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[139], line 372\u001b[0m\n\u001b[0;32m    370\u001b[0m METADATA_PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(faiss_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 372\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHUGGINGFACE_MODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(DATABASE_PATH):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m     emit_warning()\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:92\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer(\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, cache_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs\n\u001b[0;32m     94\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:308\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[0;32m    302\u001b[0m     model_name_or_path,\n\u001b[0;32m    303\u001b[0m     token,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    307\u001b[0m ):\n\u001b[1;32m--> 308\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1728\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[0;32m   1727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1728\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class(model_name_or_path, cache_dir\u001b[38;5;241m=\u001b[39mcache_folder, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:78\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[0;32m     77\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir, backend, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:138\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[1;34m(self, model_name_or_path, config, cache_dir, backend, **model_args)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    139\u001b[0m         model_name_or_path, config\u001b[38;5;241m=\u001b[39mconfig, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args\n\u001b[0;32m    140\u001b[0m     )\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_peft_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:543\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map\n\u001b[1;32m--> 543\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_mapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m resolve_trust_remote_code(\n\u001b[0;32m    545\u001b[0m     trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n\u001b[0;32m    546\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:780\u001b[0m, in \u001b[0;36m_LazyAutoMapping.keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mkeys\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 780\u001b[0m     mapping_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    781\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_attr_from_module(key, name)\n\u001b[0;32m    782\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    784\u001b[0m     ]\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_keys \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:781\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mkeys\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    780\u001b[0m     mapping_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 781\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    784\u001b[0m     ]\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_keys \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:777\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[1;34m(self, model_type, attr)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:697\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[1;34m(module, attr)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m transformers_module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;241m!=\u001b[39m transformers_module:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:157\u001b[0m, in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:172\u001b[0m, in \u001b[0;36m_get_module_lock\u001b[1;34m(name)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2144\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2144\u001b[0m         stb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2145\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_offset\u001b[49m\n\u001b[0;32m   2146\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2148\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py:1435\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[1;32m-> 1435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py:1326\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py:1173\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1176\u001b[0m colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py:1063\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1061\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[0;32m   1062\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1063\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m   1064\u001b[0m )\n\u001b[0;32m   1066\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py:1138\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1138\u001b[0m     max_len \u001b[38;5;241m=\u001b[39m \u001b[43mget_line_number_of_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py:184\u001b[0m, in \u001b[0;36mget_line_number_of_frame\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m first \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(lines)\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcount_lines_in_py_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py:150\u001b[0m, in \u001b[0;36mcount_lines_in_py_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m--> 150\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py:150\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m--> 150\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1252.py:22\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIncrementalDecoder\u001b[39;00m(codecs\u001b[38;5;241m.\u001b[39mIncrementalDecoder):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2165\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_showtraceback(etype, value, stb)\n\u001b[0;32m   2164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m-> 2165\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2088\u001b[0m, in \u001b[0;36mInteractiveShell.get_exception_only\u001b[1;34m(self, exc_tuple)\u001b[0m\n\u001b[0;32m   2083\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;124;03mReturn as a string (ending with a newline) the exception that\u001b[39;00m\n\u001b[0;32m   2085\u001b[0m \u001b[38;5;124;03mjust occurred, without any traceback.\u001b[39;00m\n\u001b[0;32m   2086\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2087\u001b[0m etype, value, tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_exc_info(exc_tuple)\n\u001b[1;32m-> 2088\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2089\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(msg)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\traceback.py:140\u001b[0m, in \u001b[0;36mformat_exception_only\u001b[1;34m(etype, value)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_exception_only\u001b[39m(etype, value):\n\u001b[0;32m    125\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format the exception part of a traceback.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    The arguments are the exception type and value such as given by\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m \n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mTracebackException\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mformat_exception_only())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\exceptiongroup\\_formatting.py:179\u001b[0m, in \u001b[0;36mPatchedTracebackException.__init__\u001b[1;34m(self, exc_type, exc_value, exc_traceback, limit, lookup_lines, capture_locals, compact, _seen)\u001b[0m\n\u001b[0;32m    172\u001b[0m     need_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    174\u001b[0m     e\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m need_context\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mid\u001b[39m(e\u001b[38;5;241m.\u001b[39m__context__) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _seen\n\u001b[0;32m    178\u001b[0m ):\n\u001b[1;32m--> 179\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43mPatchedTracebackException\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__context__\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__context__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__context__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__traceback__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlookup_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlookup_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_locals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_seen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_seen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\exceptiongroup\\_formatting.py:96\u001b[0m, in \u001b[0;36mPatchedTracebackException.__init__\u001b[1;34m(self, exc_type, exc_value, exc_traceback, limit, lookup_lines, capture_locals, compact, _seen)\u001b[0m\n\u001b[0;32m     93\u001b[0m     _seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     94\u001b[0m _seen\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exc_value))\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack \u001b[38;5;241m=\u001b[39m \u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwalk_tb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_traceback\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlookup_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlookup_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_locals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexc_type \u001b[38;5;241m=\u001b[39m exc_type\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Capture now to permit freeing resources: only complication is in the\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# unofficial API _format_final_exc_line\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\traceback.py:362\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    359\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    360\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals))\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m--> 362\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "###################################\n",
    "#           LIBRARIES             #\n",
    "###################################\n",
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import uuid\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "import textdistance\n",
    "import httpx\n",
    "from py4j.java_gateway import JavaGateway\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET  # For parsing the Ecore file\n",
    "\n",
    "from codecarbon import EmissionsTracker  # Import CodeCarbon\n",
    "\n",
    "# LangChain and related libraries\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.llms import Ollama\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.tools.base import Tool\n",
    "from typing import Callable\n",
    "\n",
    "from pytextdist.edit_distance import levenshtein_distance, levenshtein_similarity\n",
    "from pytextdist.vector_similarity import cosine_similarity\n",
    "\n",
    "###################################\n",
    "#         CONFIGURATION           #\n",
    "###################################\n",
    "\n",
    "# Configuration file paths (llm_config_anthropic, llm_config_google, llm_config_groq, llm_config_mistral, llm_config_ollama, llm_config_openai)\n",
    "CONFIG_FILE = \"config/llm_config_mistral.json\"\n",
    "MODELS_FILE = \"config/llm_models.json\"\n",
    "CONFIG_RAG_FILE = \"config/llm_config_openai_rag.json\"\n",
    "CONFIG_RAG_TAVILY_FILE = \"config/secrets-master-llm.json\"\n",
    "VECTOR_DB_TYPE = \"FAISS\" # FAISS, CHROMA\n",
    "CSV_FILE_PATH = \"config/BASE_URL.csv\"\n",
    "LLM_TYPE = 'Others' # 'Others', 'Ollama' (\n",
    "RAG_CHAT = 'OpenAI' # 'OpenAI', 'LangChain' (Same Model as the model Generation)\n",
    "\n",
    "# Define an array with all the topics/tools for retrieval\n",
    "vectorstore_topics = [\n",
    "    # \"CAEX/AutomationML\",\n",
    "    \"BPMN Designer\",\n",
    "    \"HEPSYCODE\",\n",
    "    # \"Additional Tool 1\",\n",
    "    # \"Additional Tool 2\",\n",
    "    # Add more topics as needed\n",
    "]\n",
    "\n",
    "###################################\n",
    "#         UTILITY FUNCTIONS       #\n",
    "###################################\n",
    "\n",
    "# Function to load configuration from a JSON file\n",
    "def load_config(config_file):\n",
    "    try:\n",
    "        with open(config_file, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Configuration file {config_file} not found.\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Function to load file content\n",
    "def load_file_content(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found.\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to save content to a file\n",
    "def save_to_file(file_path, content):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(content)\n",
    "\n",
    "# Function to save metadata to a file (in JSON format)\n",
    "def save_metadata(file_path, metadata):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(metadata, file, indent=4)\n",
    "\n",
    "###################################\n",
    "#         LLM CONFIGURATION       #\n",
    "###################################\n",
    "\n",
    "# Load LLM configuration\n",
    "config = load_config(CONFIG_FILE)\n",
    "models_config = load_config(MODELS_FILE)\n",
    "\n",
    "# Extract LLM parameters from configuration\n",
    "LLM = config.get(\"llm\")\n",
    "if not LLM:\n",
    "    raise ValueError(\"LLM name must be specified in the configuration file.\")\n",
    "\n",
    "PRICE_PER_INPUT_TOKEN = config.get(\"price_per_input_token\")\n",
    "print(f\"PRICE_PER_INPUT_TOKEN: {PRICE_PER_INPUT_TOKEN} $\")\n",
    "PRICE_PER_OUTPUT_TOKEN = config.get(\"price_per_output_token\")\n",
    "print(f\"PRICE_PER_OUTPUT_TOKEN: {PRICE_PER_OUTPUT_TOKEN} $\")\n",
    "temperature = config.get(\"temperature\")\n",
    "max_retries = config.get(\"max_retries\")\n",
    "api_key = config.get(\"api_keys\", {}).get(LLM.lower(), None)\n",
    "base_url = config.get(\"base_url\")\n",
    "\n",
    "# Determine LLM type and initialize LLM instance\n",
    "llm_config = models_config.get(LLM, None)\n",
    "if llm_config and LLM_TYPE != 'Ollama':\n",
    "    llm_params = llm_config.get(\"params\", {})\n",
    "    llm_params[\"temperature\"] = temperature\n",
    "    llm_params[\"max_retries\"] = max_retries\n",
    "    llm_params[\"api_key\"] = api_key\n",
    "    llm_params[\"base_url\"] = base_url\n",
    "    llm_params[\"timeout\"] = 300.0 \n",
    "\n",
    "    # Dynamically initialize the LLM class\n",
    "    llm_class = eval(llm_config[\"class\"])\n",
    "    llm_LangChain = llm_class(**llm_params)\n",
    "    model_name = LLM  # Use LLM name as the model name\n",
    "    print(f\"Model Name: {model_name}\")\n",
    "    print(f\"Model Temperature: {temperature}\")\n",
    "elif LLM_TYPE == 'Ollama':\n",
    "    llm_params = llm_config.get(\"params\", {})\n",
    "    llm_params[\"temperature\"] = temperature\n",
    "    llm_params[\"base_url\"] = base_url\n",
    "    llm_params[\"timeout\"] = 300.0 \n",
    "\n",
    "    llm_class = eval(llm_config[\"class\"])\n",
    "    llm_LangChain = llm_class(**llm_params)\n",
    "    model_name = LLM\n",
    "    print(f\"Model Name: {model_name}\")\n",
    "    print(f\"Model Temperature: {temperature}\")\n",
    "else:\n",
    "    raise ValueError(f\"Model configuration for '{LLM}' not found in {MODELS_FILE}.\")\n",
    "\n",
    "###################################\n",
    "#     FEW-SHOT CONFIGURATION      #\n",
    "###################################\n",
    "\n",
    "# Define file paths for static resources and output directories\n",
    "example_model_path = \"../../01_MSE/BPMN-Designer/BPMN-Designer-Models/D1/BPMN-MIWG/c50.bpmn\"\n",
    "\n",
    "# 01_MSE/BPMN-Designer/BPMN-Designer-Model-Description/D1/BPMN-HD\n",
    "\n",
    "# Path to the metamodel (Ecore file)\n",
    "metamodel_path = \"../../01_MSE/BPMN-Designer/workspace/org.obeonetwork.dsl.bpmn2/model/BPMN20.ecore\"\n",
    "\n",
    "# Path to the model description folders (bpmn file)\n",
    "base_model_path = \"../../01_MSE/BPMN-Designer/BPMN-Designer-Model-Description/D1/BPMN-HD/\"\n",
    "\n",
    "# Path to the Output Directory\n",
    "base_output_dir = f\"D2-BPMN-Designer/LLM-{model_name.lower()}-{temperature}\"\n",
    "base_output_json_dir = f\"D2-BPMN-Designer/LLM-{model_name.lower()}-{temperature}/JSON\"\n",
    "\n",
    "# Profiling Folder\n",
    "PROFILING_FOLDER = f\"D2-BPMN-Designer/LLM-{model_name.lower()}-{temperature}/JSON\"\n",
    "if not os.path.exists(PROFILING_FOLDER):\n",
    "    os.makedirs(PROFILING_FOLDER)\n",
    "PROFILING_CSV_FILE = os.path.join(PROFILING_FOLDER, \"profiling.csv\")\n",
    "\n",
    "# CodeCarbon Folder\n",
    "CODECARBON_FOLDER  = f\"D2-BPMN-Designer/LLM-{model_name.lower()}-{temperature}/JSON\"\n",
    "if not os.path.exists(CODECARBON_FOLDER ):\n",
    "    os.makedirs(CODECARBON_FOLDER )\n",
    "PROFILING_CSV_FILE = os.path.join(PROFILING_FOLDER, \"codecarbon_summary.csv\")\n",
    "\n",
    "# Folder to save evaluation results per file\n",
    "EVALUATION_FOLDER = f\"D2-BPMN-Designer/LLM-{model_name.lower()}-{temperature}/JSON\"\n",
    "if not os.path.exists(EVALUATION_FOLDER):\n",
    "    os.makedirs(EVALUATION_FOLDER)\n",
    "\n",
    "# Global variable to force context regeneration regardless of REFINED_CONTEXT_PATH presence\n",
    "FORCE_CONTEXT_GEN = False  # Set to True to force context generation even if REFINED_CONTEXT_PATH exists\n",
    "\n",
    "# File path to save the refined context (persistent cache)\n",
    "REFINED_CONTEXT_PATH = f\"D2-BPMN-Designer/LLM-{model_name.lower()}-{temperature}/BPMN-Designer_refined_context.json\"\n",
    "#REFINED_CONTEXT_PATH = \"config/BPMN-Designer_refined_context.json\"\n",
    "# REFINED_CONTEXT_PATH = \"config/CAEX_refined_context.json\"\n",
    "# REFINED_CONTEXT_PATH = \"config/BPMN_Designer_refined_context.json\"\n",
    "\n",
    "###################################\n",
    "#         GLOBAL PROFILING        #\n",
    "###################################\n",
    "\n",
    "# Global list to collect CodeCarbon metrics for each node call (per file)\n",
    "cc_metrics_for_file = []  # This will be reset for each file\n",
    "\n",
    "# Global list for overall CodeCarbon summary per file\n",
    "cc_summary_records = []\n",
    "\n",
    "# Global list to save profiling data\n",
    "profiling_records = []\n",
    "\n",
    "###################################\n",
    "#      TIMING NODE PROFILING      #\n",
    "###################################\n",
    "\n",
    "def timing_profile_node(func):\n",
    "    \"\"\"\n",
    "    Decorator to profile a node function.\n",
    "    Appends a record with the node name and its execution time (in seconds) to profiling_records.\n",
    "    \"\"\"\n",
    "    def wrapper(state, *args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(state, *args, **kwargs)\n",
    "        end = time.time()\n",
    "        elapsed = end - start\n",
    "        profiling_records.append({\"node\": func.__name__, \"execution_time\": elapsed})\n",
    "        print(f\"[Profiling] {func.__name__} took {elapsed:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "###################################\n",
    "#    CODECARBON NODE DECORATOR    #\n",
    "###################################\n",
    "\n",
    "# os.environ[\"CODECARBON_API_KEY\"] = \"CODECARBON_API_KEY\"\n",
    "# os.environ[\"CODECARBON_API_URL\"] = \"https://api.codecarbon.io\"\n",
    "# os.environ[\"CODECARBON_EXPERIMENT_ID\"] = \"UUID\"\n",
    "\n",
    "def cc_profile_node(func):\n",
    "    \"\"\"\n",
    "    Decorator that wraps a node function with CodeCarbon tracking.\n",
    "    It starts a tracker before calling the node and stops it right after.\n",
    "    The resulting metrics are appended to the global cc_metrics_for_file list.\n",
    "    \"\"\"\n",
    "    def wrapper(state, *args, **kwargs):\n",
    "        # Create a CodeCarbon tracker for this node\n",
    "        tracker = EmissionsTracker(\n",
    "            project_name=f\"cc_{func.__name__}\",\n",
    "            measure_power_secs=1,\n",
    "            output_dir=CODECARBON_FOLDER,  # You can adjust output_dir as needed (\".\")\n",
    "            allow_multiple_runs=True\n",
    "            # api_call_interval=4,\n",
    "            # experiment_id=experiment_id,\n",
    "            # save_to_api=True\n",
    "        )\n",
    "        tracker.start()\n",
    "        result = func(state, *args, **kwargs)\n",
    "        emissions = tracker.stop()\n",
    "        # Try to extract detailed metrics if available (from the internal attribute)\n",
    "        if hasattr(tracker, \"_final_emissions_data\"):\n",
    "            metrics = tracker._final_emissions_data\n",
    "        else:\n",
    "            metrics = {\"total_emissions\": emissions}\n",
    "        # Append the node's CodeCarbon metrics to the global list\n",
    "        cc_metrics_for_file.append({\n",
    "            \"node\": func.__name__,\n",
    "            **metrics  # Flatten the metrics dictionary\n",
    "        })\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "###################################\n",
    "#       PROFILE & CC DECORATORS   #\n",
    "###################################\n",
    "\n",
    "# (Assuming you already have a @profile_node decorator for timing, as in your code.)\n",
    "# Here we combine both decorators so that each node is profiled for time and CodeCarbon metrics.\n",
    "# The order of decorators means that cc_profile_node will wrap the function first.\n",
    "def profile_node(func):\n",
    "    return timing_profile_node(cc_profile_node(func))\n",
    "\n",
    "###################################\n",
    "#       LOAD URLS FROM CSV        #\n",
    "###################################\n",
    "\n",
    "def load_urls_from_csv(csv_file_path):\n",
    "    urls = []\n",
    "    try:\n",
    "        with open(csv_file_path, 'r', newline='', encoding='utf-8') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            for row in reader:\n",
    "                if row:  # Ensure the row is not empty\n",
    "                    urls.append(row[0].strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file '{csv_file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "    return urls\n",
    "\n",
    "# Load base URLs for the vector database from CSV\n",
    "BASE_URLS = load_urls_from_csv(CSV_FILE_PATH)\n",
    "if not BASE_URLS:\n",
    "    raise ValueError(\"No URLs were loaded from the CSV file.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(BASE_URLS)} URLs from '{CSV_FILE_PATH}'.\")\n",
    "\n",
    "###################################\n",
    "# RAG AGENT SETUP (Chroma/FAISS)  #\n",
    "###################################\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "if VECTOR_DB_TYPE == \"CHROMA\":\n",
    "    # Directory for persisting the Chroma vector store.\n",
    "    CHROMA_PERSIST_DIR = \"chroma_db\"\n",
    "    \n",
    "    # Load RAG configuration\n",
    "    config_rag = load_config(CONFIG_RAG_FILE)\n",
    "    api_key_rag = config_rag.get(\"api_keys\", {}).get(LLM.lower(), None)\n",
    "    \n",
    "    # Initialize OpenAIEmbeddings\n",
    "    embd = OpenAIEmbeddings(openai_api_key=api_key_rag)\n",
    "    \n",
    "    # Build or load the Chroma vector store\n",
    "    if os.path.exists(CHROMA_PERSIST_DIR) and os.listdir(CHROMA_PERSIST_DIR):\n",
    "        print(\"Loading existing Chroma vector store from disk...\")\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            embedding_function=embd,\n",
    "            collection_name=\"rag-chroma\"\n",
    "        )\n",
    "        retriever = vectorstore.as_retriever()\n",
    "    else:\n",
    "        print(\"Creating new Chroma vector store...\")\n",
    "        docs = [WebBaseLoader(url).load() for url in BASE_URLS]\n",
    "        docs_list = [item for sublist in docs for item in sublist]\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=500, chunk_overlap=0\n",
    "        )\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=doc_splits,\n",
    "            collection_name=\"rag-chroma\",\n",
    "            embedding=embd,\n",
    "            persist_directory=CHROMA_PERSIST_DIR\n",
    "        )\n",
    "        retriever = vectorstore.as_retriever()\n",
    "elif VECTOR_DB_TYPE == \"FAISS\":\n",
    "\n",
    "    # Load RAG configuration\n",
    "    config_rag = load_config(CONFIG_RAG_FILE)\n",
    "    api_key_rag = config_rag.get(\"api_keys\", {}).get(LLM.lower(), None)\n",
    "    \n",
    "    EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"  \n",
    "    HUGGINGFACE_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    \n",
    "    faiss_folder = \"faiss\"\n",
    "    if not os.path.exists(faiss_folder):\n",
    "        os.makedirs(faiss_folder)\n",
    "        print(f\"Folder '{faiss_folder}' created.\")\n",
    "    else:\n",
    "        print(f\"The folder '{faiss_folder}' already exists.\")\n",
    "    \n",
    "    DATABASE_PATH = os.path.join(faiss_folder, \"faiss_index.index\")\n",
    "    METADATA_PATH = os.path.join(faiss_folder, \"metadata.json\")\n",
    "    \n",
    "    embedding = HuggingFaceEmbeddings(model_name=HUGGINGFACE_MODEL_NAME)\n",
    "    \n",
    "    if os.path.exists(DATABASE_PATH):\n",
    "        print(\"Loading existing FAISS index from disk...\")\n",
    "        vectorstore = FAISS.load_local(DATABASE_PATH, embedding, allow_dangerous_deserialization=True)\n",
    "        if os.path.exists(METADATA_PATH):\n",
    "            with open(METADATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "    else:\n",
    "        print(\"Creating new FAISS vector store...\")\n",
    "        from langchain_community.document_loaders import WebBaseLoader\n",
    "        docs = [WebBaseLoader(url).load() for url in BASE_URLS]\n",
    "        docs_list = [item for sublist in docs for item in sublist]\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=0)\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        vectorstore = FAISS.from_documents(doc_splits, embedding)\n",
    "        vectorstore.save_local(DATABASE_PATH)\n",
    "        metadata = [doc.metadata for doc in doc_splits]\n",
    "        with open(METADATA_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "        \n",
    "    retriever = vectorstore.as_retriever()    \n",
    "\n",
    "###################################\n",
    "#         ROUTER NODE             #\n",
    "###################################\n",
    "\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Data model for routing the user query\n",
    "class RouteQuery(BaseModel):\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Route the user query to either a vectorstore or web search.\"\n",
    "    )\n",
    "\n",
    "# Initialize RAG LLM and router\n",
    "LLM_RAG = config_rag.get(\"llm\")\n",
    "LLM_RAG_TEMP = config_rag.get(\"temperature\")\n",
    "\n",
    "if RAG_CHAT == 'OpenAI':\n",
    "    llm_rag = ChatOpenAI(model=LLM_RAG, temperature=LLM_RAG_TEMP)\n",
    "elif RAG_CHAT == 'Ollama':\n",
    "    llm_rag = OllamaFunctions(model=LLM) \n",
    "elif RAG_CHAT == 'LangChain':\n",
    "    llm_rag = llm_LangChain\n",
    "\n",
    "structured_llm_router = llm_rag.with_structured_output(RouteQuery)\n",
    "\n",
    "# Join the topics into a single string, separated by commas\n",
    "topics_str = \", \".join(vectorstore_topics)\n",
    "\n",
    "# Create router prompt\n",
    "router_system_prompt = (\n",
    "    \"You are an expert at routing user queries to either a vectorstore or web search. \"\n",
    "    \"The vectorstore contains documents related to {topics_str}.\"\n",
    "    \"Use the vectorstore for questions on these topics; otherwise, use web search.\"\n",
    "    \"Based on the query, respond with a JSON object that contains a key 'datasource'\"\n",
    "    \"whose value is either 'vectorstore' or 'web_search'. Do not include any additional keys or text.\"\n",
    ")\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", router_system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "question_router = route_prompt | structured_llm_router\n",
    "\n",
    "###################################\n",
    "#      RETRIEVAL GRADER NODE      #\n",
    "###################################\n",
    "\n",
    "# Data model for grading document relevance\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Indicates whether the document is relevant ('yes' or 'no').\"\n",
    "    )\n",
    "\n",
    "structured_llm_grader = llm_rag.with_structured_output(GradeDocuments)\n",
    "\n",
    "grader_system_prompt = (\n",
    "    \"You are a grader assessing the relevance of a retrieved document to a user query. \"\n",
    "    \"If the document contains keywords or semantic content related to the user query, grade it as relevant. \"\n",
    "    \"Output a binary score 'yes' or 'no'.\"\n",
    ")\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grader_system_prompt),\n",
    "        (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser query:\\n{question}\"),\n",
    "    ]\n",
    ")\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "###################################\n",
    "#        GENERATION CHAIN         #\n",
    "###################################\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "# Set up in-memory cache to avoid repeating expensive LLM calls.\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "\"\"\"\n",
    "context_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You are an expert in information retrieval and content synthesis. Your task is to refine and enhance context \"\n",
    "                \"from multiple sources by generating a cohesive, well-structured, and detailed context that combines information \"\n",
    "                \"from various retrieved documents.\\n\\n\"\n",
    "                \"Responsibilities:\\n\"\n",
    "                \"1. Synthesize information from multiple sources into a unified explanation.\\n\"\n",
    "                \"2. Expand on the query with relevant details from the retrieved content.\\n\"\n",
    "                \"3. Format the refined context with clear structure and professional language.\\n\"\n",
    "                \"4. Incorporate metadata for traceability.\\n\"\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            (\n",
    "                \"Question: {question}\\n\\n\"\n",
    "                \"The following are the retrieved documents and metadata:\\n\\n{context}\\n\\n\"\n",
    "                \"Using this information, generate a refined and comprehensive context.\"\n",
    "            )\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Load static content files\n",
    "metamodel_text = load_file_content(metamodel_path)\n",
    "\n",
    "context_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You are a domain model expert with deep knowledge of metamodeling and semantic model interpretation.\"\n",
    "                \"Your task is to analyze a given Ecore metamodel and extract a structured and comprehensive domain context.\"\n",
    "                \"This context will be used as background knowledge to guide synthetic model generation.\\n\\n\"\n",
    "                \"Instructions:\\n\"\n",
    "                \"1. Identify and describe the key domain concepts (classes, attributes, references, enumerations).\\n\"\n",
    "                \"2. Explain the relationships and constraints implied by the metamodel structure.\\n\"\n",
    "                \"3. Enrich the context with relevant external domain knowledge.\\n\"\n",
    "                \"4. Structure the output clearly with sections like 'Domain Overview', 'Key Concepts', 'Relationships', 'Behavioral Semantics', and 'External Domain Background'.\\n\"\n",
    "                \"5. When possible, infer the real-world domain (e.g., hardware modeling, system behavior, message passing) and include related terminology.\\n\"\n",
    "                \"6. Use professional and academic language, suitable for expert systems.\\n\"\n",
    "                \"7. Incorporate any relevant metadata such as names, types, multiplicities, and containment relationships.\"\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            (\n",
    "                \"Here is the Ecore metamodel to analyze:\\n\\n\"\n",
    "                \"{metamodel_text}\\n\\n\"\n",
    "                \"Question: {question}\\n\\n\"\n",
    "                \"The following are the retrieved documents and metadata:\\n\\n{context}\\n\\n\"\n",
    "                \"Using this, generate a refined and detailed context about the domain it represents.\"\n",
    "            )\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "if RAG_CHAT == 'OpenAI':\n",
    "    llm_for_context = ChatOpenAI(model=LLM_RAG, temperature=LLM_RAG_TEMP)\n",
    "elif RAG_CHAT == 'LangChain':\n",
    "    llm_for_context = llm_LangChain\n",
    "    \n",
    "rag_chain = context_prompt_template | llm_for_context | StrOutputParser()\n",
    "\n",
    "###################################\n",
    "#     HALLUCINATION GRADER        #\n",
    "###################################\n",
    "\n",
    "# Data model for grading hallucination\n",
    "class GradeHallucinations(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Indicates if the answer is grounded in facts ('yes' or 'no').\"\n",
    "    )\n",
    "\n",
    "structured_llm_hallucination = llm_rag.with_structured_output(GradeHallucinations)\n",
    "\n",
    "hallucination_system_prompt = (\n",
    "    \"You are a grader assessing whether the LLM generation is grounded in the retrieved facts. \"\n",
    "    \"Output a binary score 'yes' if the answer is supported by the facts, otherwise 'no'.\"\n",
    ")\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", hallucination_system_prompt),\n",
    "        (\"human\", \"Facts:\\n\\n{documents}\\n\\nLLM Generation:\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "hallucination_grader = hallucination_prompt | structured_llm_hallucination\n",
    "\n",
    "###################################\n",
    "#         ANSWER GRADER           #\n",
    "###################################\n",
    "\n",
    "\"\"\"\n",
    "# Data model for grading answer relevance\n",
    "class GradeAnswer(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Indicates if the answer addresses the question ('yes' or 'no').\"\n",
    "    )\n",
    "\n",
    "structured_llm_answer = llm_rag.with_structured_output(GradeAnswer)\n",
    "\n",
    "answer_system_prompt = (\n",
    "    \"You are a grader assessing whether an LLM-generated answer addresses the user query. \"\n",
    "    \"Output a binary score 'yes' if it does, otherwise 'no'.\"\n",
    ")\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", answer_system_prompt),\n",
    "        (\"human\", \"User Query:\\n{question}\\n\\nLLM Generation:\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "answer_grader = answer_prompt | structured_llm_answer\n",
    "\"\"\"\n",
    "\n",
    "###################################\n",
    "#       QUESTION REWRITER         #\n",
    "###################################\n",
    "\n",
    "rewrite_system_prompt = (\n",
    "    \"You are a question rewriter. Given an input question, produce an improved version optimized for vectorstore retrieval. \"\n",
    "    \"Focus on the underlying semantic intent.\"\n",
    ")\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rewrite_system_prompt),\n",
    "        (\"human\", \"Original question:\\n{question}\\n\\nRewrite the question:\"),\n",
    "    ]\n",
    ")\n",
    "question_rewriter = rewrite_prompt | llm_for_context | StrOutputParser()\n",
    "\n",
    "###################################\n",
    "#           WEB SEARCH            #\n",
    "###################################\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "config_tavily = load_config(CONFIG_RAG_TAVILY_FILE)\n",
    "os.environ[\"TAVILY_API_KEY\"] = config_tavily.get(\"tavily_api_key\")\n",
    "web_search_tool = TavilySearchResults(k=3)\n",
    "\n",
    "###################################\n",
    "#       GRAPH STATE DEFINITION    #\n",
    "###################################\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict, total=False): \n",
    "    # Retrieval and model generation branch keys\n",
    "    question: str                         # The user's search question or refined query\n",
    "    generation: str                       # The generated answer or refined query output\n",
    "    documents: List[Any]                  # List of retrieved documents\n",
    "    file_name: str                        # For model generation node\n",
    "    context_llm: str                      # The refined context generated by the LLM\n",
    "    model_status: str                     # Status of the processing model\n",
    "    model_out: str                        # The output model\n",
    "    metadata: Dict[str, Any]              # Additional metadata related to the process\n",
    "    branch: str                           # Indicates the branch: \"retrieve\" or \"web_search\"\n",
    "    evaluation_metrics: Dict[str, float]  # Metrics evaluating the generated results\n",
    "    bert_score: Dict[str, float]          # BERTScore metrics for evaluating document support\n",
    "    web_bert_score: Dict[str, float]      # BERTScore metrics for evaluating web search branch results\n",
    "    skip_router: bool                     # If True, skip the routing and proceed directly to cache_context_node\n",
    "    model_val: bool                       # Flag indicating whether the model should be refined or not\n",
    "    validation_attempts: int              # Number of validation/correction attempts\n",
    "    next_step: str                        # Next transition key for conditional routing\n",
    "    val_cycles: int                     # Number of generation-validation cycles\n",
    "\n",
    "###################################\n",
    "# FUNCTION: GENERATE QUERY FROM METAMODEL #\n",
    "###################################\n",
    "\n",
    "\"\"\"\n",
    "@profile_node\n",
    "def generate_query_from_metamodel(metamodel_path):\n",
    "    # Reads the metamodel (Ecore file), extracts basic information (package name, nsURI, and classifiers),\n",
    "    # and constructs a query based solely on the extracted information.\n",
    "    try:\n",
    "        tree = ET.parse(metamodel_path)\n",
    "        root = tree.getroot()\n",
    "        # The root is typically an EPackage with attributes\n",
    "        package_name = root.attrib.get(\"name\", \"UnknownPackage\")\n",
    "        ns_uri = root.attrib.get(\"nsURI\", \"Unknown nsURI\")\n",
    "        # Extract all classifiers (e.g., EClass, EEnum, etc.)\n",
    "        classifiers = []\n",
    "        for classifier in root.findall(\"{http://www.eclipse.org/emf/2002/Ecore}eClassifiers\"):\n",
    "            classifiers.append(classifier.attrib.get(\"name\", \"UnnamedClassifier\"))\n",
    "        classifiers_str = \", \".join(classifiers) if classifiers else \"None\"\n",
    "        # Build a generic query based solely on the metamodel information\n",
    "        query = (\n",
    "            f\"Metamodel Analysis Query:\\n\"\n",
    "            f\"Package Name: {package_name}\\n\"\n",
    "            f\"Namespace URI: {ns_uri}\\n\"\n",
    "            f\"Classifiers: {classifiers_str}\\n\"\n",
    "            \"Based solely on the metamodel information provided above, generate a context for a tool based on this metamodel. \"\n",
    "            \"The context should include the tool's name, which must be directly derived from the package name.\"\n",
    "        )\n",
    "        return query\n",
    "    except Exception as e:\n",
    "        print(\"Error generating query from metamodel:\", e)\n",
    "        return \"Metamodel analysis query could not be generated.\"\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "@profile_node\n",
    "def generate_query_from_metamodel(metamodel_path):\n",
    "    try:\n",
    "        tree = ET.parse(metamodel_path)\n",
    "        root = tree.getroot()\n",
    "        package_name = root.attrib.get(\"name\", \"UnknownPackage\")\n",
    "        ns_uri = root.attrib.get(\"nsURI\", \"Unknown nsURI\")\n",
    "        classifiers = [classifier.attrib.get(\"name\", \"UnnamedClassifier\")\n",
    "                       for classifier in root.findall(\"{http://www.eclipse.org/emf/2002/Ecore}eClassifiers\")]\n",
    "        classifiers_str = \", \".join(classifiers) if classifiers else \"None\"\n",
    "        # Genera una query generica arricchita con termini descrittivi e sinonimi\n",
    "        query = (\n",
    "            f\"Metamodel Analysis Query:\\n\"\n",
    "            f\"Package Name: {package_name}\\n\"\n",
    "            f\"Namespace URI: {ns_uri}\\n\"\n",
    "            f\"Classifiers: {classifiers_str}\\n\"\n",
    "            \"Generate a detailed and generic query to retrieve relevant documents related to tools or systems \"\n",
    "            \"that conform to the domain represented by this metamodel. Include additional descriptive keywords, synonyms, \"\n",
    "            \"and contextual terms that capture the essence of the domain. Ensure that the tool's name is directly derived \"\n",
    "            \"from the package name.\"\n",
    "        )\n",
    "        return query\n",
    "    except Exception as e:\n",
    "        print(\"Error generating query from metamodel:\", e)\n",
    "        return \"Metamodel analysis query could not be generated.\"\n",
    "\"\"\"\n",
    "\n",
    "def generate_query_from_metamodel(metamodel_path):\n",
    "    try:\n",
    "        tree = ET.parse(metamodel_path)\n",
    "        root = tree.getroot()\n",
    "        package_name = root.attrib.get(\"name\", \"UnknownPackage\")\n",
    "        ns_uri = root.attrib.get(\"nsURI\", \"Unknown nsURI\")\n",
    "        classifiers = [classifier.attrib.get(\"name\", \"UnnamedClassifier\")\n",
    "                       for classifier in root.findall(\"{http://www.eclipse.org/emf/2002/Ecore}eClassifiers\")]\n",
    "        classifiers_str = \", \".join(classifiers) if classifiers else \"None\"\n",
    "        # Creazione di una query specifica per BPMN Designer\n",
    "        query = (\n",
    "            f\"BPMN Designer Query:\\n\"\n",
    "            f\"Package Name: {package_name}\\n\"\n",
    "            f\"Namespace URI: {ns_uri}\\n\"\n",
    "            f\"Classifiers: {classifiers_str}\\n\"\n",
    "            \"Based on the above metamodel information, generate a comprehensive query to retrieve documents that \"\n",
    "            \"explain or demonstrate BPMN modeling concepts and the use of BPMN Designer. Include detailed keywords such as \"\n",
    "            \"'business process modeling', 'workflow design', 'BPMN diagram', 'gateway', 'task', 'event', and 'sequence flow'. \"\n",
    "            \"Ensure that the tool's name is directly derived from the package name.\"\n",
    "        )\n",
    "        return query\n",
    "    except Exception as e:\n",
    "        print(\"Error generating query from metamodel:\", e)\n",
    "        return \"BPMN Designer query could not be generated.\"\n",
    "\n",
    "# The query for generating the context is now created based on the metamodel\n",
    "# question = generate_query_from_metamodel(metamodel_path)\n",
    "# print(question)\n",
    "\n",
    "###################################\n",
    "#        EVALUATION NODES         #\n",
    "###################################\n",
    "\n",
    "# (1) LLM-based Evaluation for RAG output (vectorstore branch)\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class RAGEvaluationMetrics(BaseModel):\n",
    "    faithfulness: float = Field(..., description=\"Score (0-1) indicating how faithful the answer is to the facts.\")\n",
    "    answer_relevance: float = Field(..., description=\"Score (0-1) indicating how well the answer addresses the question.\")\n",
    "    context_precision: float = Field(..., description=\"Score (0-1) representing the precision of the context used.\")\n",
    "    context_accuracy: float = Field(..., description=\"Score (0-1) representing the accuracy of the retrieved context.\")\n",
    "    context_recall: float = Field(..., description=\"Score (0-1) representing the recall of the context.\")\n",
    "    context_f1: float = Field(..., description=\"Score (0-1) representing the F1 measure of the context.\")\n",
    "\n",
    "@profile_node\n",
    "def evaluate_rag_output(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Node to evaluate the RAG output (vectorstore branch) based on metrics such as:\n",
    "    Faithfulness, Answer Relevance, Context Precision, Context Accuracy,\n",
    "    Context Recall, and Context F1.\n",
    "    \"\"\"\n",
    "    print(\"--- EVALUATE RAG OUTPUT METRICS ---\")\n",
    "    question_val = state.get(\"question\", \"\")\n",
    "    generation = state.get(\"generation\", \"\")\n",
    "    documents = state.get(\"documents\", [])\n",
    "    context_text = \"\\n\".join([doc.page_content for doc in documents]) if documents else \"\"\n",
    "    \n",
    "    eval_input = {\n",
    "         \"question\": question_val,\n",
    "         \"generation\": generation,\n",
    "         \"context\": context_text\n",
    "    }\n",
    "    \n",
    "    eval_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", (\n",
    "            \"You are an expert evaluator of RAG outputs. Evaluate the output based on the following metrics: \"\n",
    "            \"Faithfulness, Answer Relevance, Context Precision, Context Accuracy, Context Recall, and Context F1. \"\n",
    "            \"For each metric, assign a score between 0 and 1. \"\n",
    "            \"Respond in JSON format with keys: faithfulness, answer_relevance, context_precision, \"\n",
    "            \"context_accuracy, context_recall, context_f1.\"\n",
    "        )),\n",
    "        (\"user\", \"Question:\\n{question}\\n\\nGenerated Answer:\\n{generation}\\n\\nContext:\\n{context}\\n\\nProvide the evaluation:\")\n",
    "    ])\n",
    "    \n",
    "    structured_eval = llm_rag.with_structured_output(RAGEvaluationMetrics)\n",
    "    eval_chain = eval_prompt | structured_eval\n",
    "    try:\n",
    "         eval_metrics = eval_chain.invoke(eval_input)\n",
    "         state[\"evaluation_metrics\"] = eval_metrics.dict()\n",
    "         print(\"Evaluation metrics:\", state[\"evaluation_metrics\"])\n",
    "    except Exception as e:\n",
    "         print(\"Error during evaluation of RAG output:\", e)\n",
    "         state[\"evaluation_metrics\"] = {}\n",
    "    return state\n",
    "\n",
    "# (2) BERTScore Evaluation for RAG output (vectorstore branch)\n",
    "@profile_node\n",
    "def evaluate_bert_score(state: GraphState) -> GraphState:\n",
    "    print(\"--- EVALUATE BERT SCORE ---\")\n",
    "    try:\n",
    "        from bert_score import score\n",
    "    except ImportError:\n",
    "        print(\"Please install bert-score using 'pip install bert-score'\")\n",
    "        state[\"bert_score\"] = None\n",
    "        return state\n",
    "\n",
    "    candidate = state.get(\"generation\", \"\")\n",
    "    documents = state.get(\"documents\", [])\n",
    "    reference = \"\\n\".join([doc.page_content for doc in documents]) if documents else \"\"\n",
    "    \n",
    "    if not candidate or not reference:\n",
    "        print(\"Candidate or reference text is empty. Skipping BERTScore evaluation.\")\n",
    "        state[\"bert_score\"] = None\n",
    "        return state\n",
    "    \n",
    "    P, R, F1 = score([candidate], [reference], lang=\"en\", verbose=True)\n",
    "    bert_precision = P[0].item()\n",
    "    bert_recall = R[0].item()\n",
    "    bert_f1 = F1[0].item()\n",
    "    state[\"bert_score\"] = {\"precision\": bert_precision, \"recall\": bert_recall, \"f1\": bert_f1}\n",
    "    print(\"BERTScore metrics:\", state[\"bert_score\"])\n",
    "    return state\n",
    "\n",
    "# (3) LLM-based Evaluation for Web Search output\n",
    "# Here, we introduce an additional metric \"accuracy\" along with the previous ones.\n",
    "class WebEvaluationMetrics(BaseModel):\n",
    "    faithfulness: float = Field(..., description=\"Score (0-1) indicating how faithful the answer is to the web sources.\")\n",
    "    answer_relevance: float = Field(..., description=\"Score (0-1) indicating how well the answer addresses the query.\")\n",
    "    context_precision: float = Field(..., description=\"Score (0-1) representing the precision of the web search results.\")\n",
    "    context_accuracy: float = Field(..., description=\"Score (0-1) representing the accuracy of the retrieved web content.\")\n",
    "    context_recall: float = Field(..., description=\"Score (0-1) representing the recall of relevant web information.\")\n",
    "    context_f1: float = Field(..., description=\"Score (0-1) representing the F1 measure of the web search results.\")\n",
    "    accuracy: float = Field(..., description=\"Score (0-1) indicating the overall accuracy of the generated context based on web sources.\")\n",
    "\n",
    "@profile_node\n",
    "def evaluate_web_search_output(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Node to evaluate the output of the web search branch.\n",
    "    It uses the same metrics as the RAG evaluation plus an extra metric 'accuracy'.\n",
    "    The reference is the concatenated web search source content.\n",
    "    \"\"\"\n",
    "    print(\"--- EVALUATE WEB SEARCH OUTPUT METRICS ---\")\n",
    "    question_val = state.get(\"question\", \"\")\n",
    "    generation = state.get(\"generation\", \"\")\n",
    "    documents = state.get(\"documents\", [])\n",
    "    context_text = \"\\n\".join([doc.page_content for doc in documents]) if documents else \"\"\n",
    "    \n",
    "    eval_input = {\n",
    "         \"question\": question_val,\n",
    "         \"generation\": generation,\n",
    "         \"context\": context_text\n",
    "    }\n",
    "    \n",
    "    eval_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", (\n",
    "            \"You are an expert evaluator of web search outputs. Evaluate the output based on the following metrics: \"\n",
    "            \"Faithfulness, Answer Relevance, Context Precision, Context Accuracy, Context Recall, Context F1, and Accuracy. \"\n",
    "            \"For each metric, assign a score between 0 and 1. \"\n",
    "            \"Respond in JSON format with keys: faithfulness, answer_relevance, context_precision, \"\n",
    "            \"context_accuracy, context_recall, context_f1, accuracy.\"\n",
    "        )),\n",
    "        (\"user\", \"Question:\\n{question}\\n\\nGenerated Answer/Context:\\n{generation}\\n\\nWeb Search Sources:\\n{context}\\n\\nProvide the evaluation:\")\n",
    "    ])\n",
    "    \n",
    "    structured_eval = llm_rag.with_structured_output(WebEvaluationMetrics)\n",
    "    eval_chain = eval_prompt | structured_eval\n",
    "    try:\n",
    "         eval_metrics = eval_chain.invoke(eval_input)\n",
    "         state[\"evaluation_metrics\"] = eval_metrics.dict()\n",
    "         print(\"Web search evaluation metrics:\", state[\"evaluation_metrics\"])\n",
    "    except Exception as e:\n",
    "         print(\"Error during web search evaluation:\", e)\n",
    "         state[\"evaluation_metrics\"] = {}\n",
    "    return state\n",
    "\n",
    "# (4) BERTScore Evaluation for Web Search output\n",
    "@profile_node\n",
    "def evaluate_web_bert_score(state: GraphState) -> GraphState:\n",
    "    print(\"--- EVALUATE WEB BERT SCORE ---\")\n",
    "    try:\n",
    "        from bert_score import score\n",
    "    except ImportError:\n",
    "        print(\"Please install bert-score using 'pip install bert-score'\")\n",
    "        state[\"web_bert_score\"] = None\n",
    "        return state\n",
    "\n",
    "    candidate = state.get(\"generation\", \"\")\n",
    "    documents = state.get(\"documents\", [])\n",
    "    reference = \"\\n\".join([doc.page_content for doc in documents]) if documents else \"\"\n",
    "    \n",
    "    if not candidate or not reference:\n",
    "        print(\"Candidate or reference text is empty for web search. Skipping BERTScore evaluation.\")\n",
    "        state[\"web_bert_score\"] = None\n",
    "        return state\n",
    "    \n",
    "    P, R, F1 = score([candidate], [reference], lang=\"en\", verbose=True)\n",
    "    web_bert_precision = P[0].item()\n",
    "    web_bert_recall = R[0].item()\n",
    "    web_bert_f1 = F1[0].item()\n",
    "    state[\"web_bert_score\"] = {\"precision\": web_bert_precision, \"recall\": web_bert_recall, \"f1\": web_bert_f1}\n",
    "    print(\"Web BERTScore metrics:\", state[\"web_bert_score\"])\n",
    "    return state\n",
    "\n",
    "###################################\n",
    "#        DECIDE TO GENERATE       #\n",
    "###################################\n",
    "\n",
    "def decide_to_generate(state: GraphState) -> str:\n",
    "    print(\"--- DECIDE TO GENERATE ---\")\n",
    "    filtered_documents = state.get(\"documents\", [])\n",
    "    if not filtered_documents:\n",
    "        branch = state.get(\"branch\", \"retrieve\")  # Fallback default to \"retrieve\"\n",
    "        if branch == \"retrieve\":\n",
    "            print(\"--- No relevant documents found in vectorstore; transforming query to improve retrieval ---\")\n",
    "            return \"transform_query\"\n",
    "        else:  # branch == \"web_search\"\n",
    "            print(\"--- No documents found via web search; proceeding with generation using empty context ---\")\n",
    "            return \"generate\"\n",
    "    else:\n",
    "        print(\"--- Relevant documents found, generating answer ---\")\n",
    "        return \"generate\"\n",
    "\n",
    "###################################\n",
    "#        CACHE NODE (LangGraph)   #\n",
    "###################################\n",
    "\n",
    "@profile_node\n",
    "def cache_context_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    LangGraph node that checks if a refined context is already available.\n",
    "    If present in the state or in the file cache, it uses that value.\n",
    "    Otherwise, it generates the refined context using the rag_chain,\n",
    "    caches it (in state and on disk), and returns the state.\n",
    "    \"\"\"\n",
    "    if \"context_llm\" in state and state[\"context_llm\"]:\n",
    "        print(\"Using refined context already present in state.\")\n",
    "        return state\n",
    "\n",
    "    if os.path.isfile(REFINED_CONTEXT_PATH) and not FORCE_CONTEXT_GEN:\n",
    "        try:\n",
    "            with open(REFINED_CONTEXT_PATH, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            state[\"context_llm\"] = data.get(\"context\", \"\")\n",
    "            print(\"Loaded refined context from file cache (LangGraph node).\")\n",
    "            return state\n",
    "        except Exception as e:\n",
    "            print(\"Error loading refined context from file in cache node:\", e)\n",
    "\n",
    "    print(\"Generating refined context in LangGraph cache node...\")\n",
    "    refined_context = rag_chain.invoke({\"question\": state[\"question\"], \"context\": \"\", \"metamodel_text\": metamodel_text})\n",
    "    state[\"context_llm\"] = refined_context\n",
    "    try:\n",
    "        with open(REFINED_CONTEXT_PATH, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\"context\": refined_context}, f, indent=4, ensure_ascii=False)\n",
    "        print(\"Refined context cached to file from LangGraph node.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error caching refined context to file in cache node:\", e)\n",
    "    return state\n",
    "\n",
    "###################################\n",
    "#          GRAPH NODES            #\n",
    "###################################\n",
    "\n",
    "@profile_node\n",
    "def generate_query_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    LangGraph node that generates the query from the metamodel.\n",
    "    If the refined context file exists, skip query generation and mark state to bypass router.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(REFINED_CONTEXT_PATH) and not FORCE_CONTEXT_GEN:\n",
    "        print(\"Refined context file exists. Skipping query generation; proceeding directly to cache_context_node.\")\n",
    "        state[\"skip_router\"] = True  # Flag to skip routing\n",
    "    else:\n",
    "        state[\"skip_router\"] = False  # Flag to skip routing\n",
    "        state[\"question\"] = generate_query_from_metamodel(metamodel_path)\n",
    "        print(\"Generated query from metamodel:\", state[\"question\"])\n",
    "    return state\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "# Node: Retrieve documents using the vectorstore\n",
    "@profile_node\n",
    "def retrieve(state: GraphState) -> GraphState:\n",
    "    print(\"--- RETRIEVE ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    documents = retriever.invoke(question_val)\n",
    "    state[\"documents\"] = documents\n",
    "    return state\n",
    "\n",
    "# Node: Perform web search (remains separate)\n",
    "@profile_node\n",
    "def web_search(state: GraphState) -> GraphState:\n",
    "    print(\"--- WEB SEARCH ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    docs = web_search_tool.invoke({\"query\": question_val})\n",
    "\n",
    "    # Combine web search results into a single Document\n",
    "    \"\"\"\n",
    "    web_results_content = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    from langchain.schema import Document\n",
    "    web_results_doc = Document(page_content=web_results_content)\n",
    "    state[\"documents\"] = [web_results_doc]\n",
    "    return state\n",
    "    \"\"\"\n",
    "    # Check the type of docs and extract content accordingly.\n",
    "    if isinstance(docs, str):\n",
    "        # If docs is a string, use it directly.\n",
    "        web_results_content = docs\n",
    "    elif isinstance(docs, list):\n",
    "        # If docs is a list, check the type of its elements.\n",
    "        if docs and isinstance(docs[0], dict) and \"content\" in docs[0]:\n",
    "            web_results_content = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "        else:\n",
    "            # Assume it's a list of strings.\n",
    "            web_results_content = \"\\n\".join(docs)\n",
    "    else:\n",
    "        # Fallback: convert docs to string.\n",
    "        web_results_content = str(docs)\n",
    "    \n",
    "    from langchain.schema import Document\n",
    "    web_results_doc = Document(page_content=web_results_content)\n",
    "    state[\"documents\"] = [web_results_doc]\n",
    "    return state    \n",
    "\n",
    "# Merged Node: Generate answer using the RAG chain (used for both branches)\n",
    "def generate(state: GraphState) -> GraphState:\n",
    "    print(\"--- GENERATE (RAG) ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    context_text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "    generation = rag_chain.invoke({\"context\": context_text, \"question\": question_val, \"metamodel_text\": metamodel_text})\n",
    "    state[\"generation\"] = generation\n",
    "    return state\n",
    "\n",
    "# Node: Generate answer using web search results\n",
    "@profile_node\n",
    "def generate_web(state: GraphState) -> GraphState:\n",
    "    print(\"--- GENERATE (Web) ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question, \"metamodel_text\": metamodel_text})\n",
    "    state[\"generation\"] = generation\n",
    "    return state\n",
    "\n",
    "# Node: Grade documents for relevance\n",
    "@profile_node\n",
    "def grade_documents(state: GraphState) -> GraphState:\n",
    "    print(\"--- GRADE DOCUMENTS ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        \"\"\"\n",
    "        score = retrieval_grader.invoke({\"question\": question_val, \"document\": d.page_content})\n",
    "        if score.binary_score.lower() == \"yes\":\n",
    "            print(\"--- Document is relevant ---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"--- Document is not relevant ---\")\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        attempts = 0\n",
    "        max_attempts = 3\n",
    "        score = None\n",
    "        while attempts < max_attempts:\n",
    "            try:\n",
    "                print(\"Calling retrieval_grader with input:\", {\"question\": question_val, \"document\": d.page_content})\n",
    "                output = retrieval_grader.invoke({\"question\": question_val, \"document\": d.page_content})\n",
    "                print(\"Raw score output:\", output)\n",
    "                if isinstance(output, dict):\n",
    "                    score = output.get(\"binary_score\")\n",
    "                else:\n",
    "                    print(\"Unexpected output format:\", output)\n",
    "                    score = output.get(\"binary_score\")\n",
    "                break  # Exit loop if the call is successful\n",
    "            except httpx.HTTPStatusError as e:\n",
    "                if e.response.status_code == 429:\n",
    "                    wait_time = 10  # seconds to wait; adjust if needed\n",
    "                    print(f\"Rate limit exceeded, waiting for {wait_time} seconds before retrying...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    attempts += 1\n",
    "                else:\n",
    "                    raise  # Re-raise the error if it's not a 429\n",
    "            except httpx.ReadTimeout as e:\n",
    "                wait_time = 10  # seconds to wait; adjust if needed\n",
    "                print(f\"Read timeout occurred, waiting for {wait_time} seconds before retrying...\")\n",
    "                time.sleep(wait_time)\n",
    "                attempts += 1\n",
    "        if score is None:\n",
    "            print(\"The retrieval_grader did not return a valid result for this document, skipping it.\")\n",
    "            continue\n",
    "        if score.binary_score.lower() == \"yes\":\n",
    "            print(\"--- Document is relevant ---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"--- Document is not relevant ---\")\n",
    "        \"\"\"\n",
    "        attempts = 0\n",
    "        max_attempts = 3\n",
    "        score = None\n",
    "\n",
    "        while attempts < max_attempts:\n",
    "            try:\n",
    "                input_data = {\"question\": question_val, \"document\": d.page_content}\n",
    "                # print(f\"\\n[Grade Attempt] Input:\\n{json.dumps(input_data, indent=2)}\")\n",
    "\n",
    "                raw_output = retrieval_grader.invoke(input_data)\n",
    "                # print(f\"[Grade Output] Raw result:\\n{raw_output}\")\n",
    "\n",
    "                # Check if we received a proper object or dict\n",
    "                if hasattr(raw_output, 'binary_score'):\n",
    "                    binary = raw_output.binary_score.lower()\n",
    "                elif isinstance(raw_output, dict) and \"binary_score\" in raw_output:\n",
    "                    binary = raw_output[\"binary_score\"].lower()\n",
    "                else:\n",
    "                    print(\"Unexpected output format, skipping document.\")\n",
    "                    break  # Skip this doc\n",
    "\n",
    "                if binary == \"yes\":\n",
    "                    print(\"Document is relevant.\")\n",
    "                    filtered_docs.append(d)\n",
    "                else:\n",
    "                    print(\"Document is not relevant.\")\n",
    "\n",
    "                break  # Done with this document\n",
    "\n",
    "            except httpx.HTTPStatusError as e:\n",
    "                if e.response.status_code in [429, 502, 503]:\n",
    "                    wait_time = 10\n",
    "                    print(f\"HTTP {e.response.status_code} error, retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    attempts += 1\n",
    "                else:\n",
    "                    print(f\"Unexpected HTTP error {e.response.status_code}: {e}\")\n",
    "                    break  # Exit loop for other HTTP errors\n",
    "            except httpx.ReadTimeout as e:\n",
    "                wait_time = 15\n",
    "                print(f\"Read timeout, waiting {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "                attempts += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error grading document: {e}\")\n",
    "                break  # Don't loop forever on unknown errors\n",
    "\n",
    "    state[\"documents\"] = filtered_docs\n",
    "    return state\n",
    "    \n",
    "\n",
    "# Merged Node: Transform the query (for both branches)\n",
    "@profile_node\n",
    "def transform_query(state: GraphState) -> GraphState:\n",
    "    print(\"--- TRANSFORM QUERY (RAG) ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    better_question = question_rewriter.invoke({\"question\": question_val})\n",
    "    print(better_question)\n",
    "    state[\"question\"] = better_question\n",
    "    return state\n",
    "\n",
    "# Node: Transform the query for web search\n",
    "@profile_node\n",
    "def transform_query_web(state: GraphState) -> GraphState:\n",
    "    print(\"--- TRANSFORM QUERY (Web) ---\")\n",
    "    question = state[\"question\"]\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    print(better_question)\n",
    "    state[\"question\"] = better_question\n",
    "    return state\n",
    "\n",
    "# Conditional routing after transformation: based on branch in state\n",
    "\"\"\"\n",
    "def route_after_transform(state: GraphState) -> str:\n",
    "    if state.get(\"branch\") == \"retrieve\":\n",
    "        return \"retrieve\"\n",
    "    elif state.get(\"branch\") == \"web_search\":\n",
    "        return \"web_search\"\n",
    "    return \"retrieve\"\n",
    "\"\"\"\n",
    "\n",
    "# Node: Grade the generation against the documents and question\n",
    "@profile_node\n",
    "def grade_generation_v_documents_and_question(state: GraphState) -> str:\n",
    "    print(\"--- GRADE GENERATION ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    documents_text = \"\\n\".join([doc.page_content for doc in documents]) if documents else \"\"\n",
    "\n",
    "    # Evaluate if the generation is supported by the retrieved documents.\n",
    "    score = hallucination_grader.invoke({\"documents\": documents_text, \"generation\": generation})\n",
    "    if score.binary_score.lower() == \"yes\":\n",
    "        print(\"--- Generation is grounded in documents ---\")\n",
    "        # score_answer = answer_grader.invoke({\"question\": question_val, \"generation\": generation})\n",
    "        # if score_answer.binary_score.lower() == \"yes\":\n",
    "        #     print(\"--- Generation addresses the question ---\")\n",
    "        return \"useful\"\n",
    "        # else:\n",
    "        #     print(\"--- Generation does not address the question ---\")\n",
    "        #     return \"not useful\"\n",
    "    else:\n",
    "        print(\"--- Generation is not supported by documents, retrying ---\")\n",
    "        return \"not useful\"\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "# Node: model Generation – generate model and JSON metadata from a single file.\n",
    "# TODO: change this function to model generation and feedback with Eclipse EMF\n",
    "@profile_node\n",
    "def model_generation_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    LangGraph node for generating the model and JSON metadata\n",
    "    from a single file using the refined context.\n",
    "    \n",
    "    Expected state:\n",
    "      - \"file_name\": the file to process.\n",
    "      - \"context_llm\": the refined context.\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = state[\"file_name\"]\n",
    "    context_llm = state[\"context_llm\"]\n",
    "    \n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(base_output_dir, exist_ok=True)\n",
    "    os.makedirs(base_output_json_dir, exist_ok=True)\n",
    "    \n",
    "    output_model_path = os.path.join(base_output_dir, file_name)\n",
    "    metadata_path = os.path.join(base_output_json_dir, file_name)\n",
    "\n",
    "    attempts = state.get(\"validation_attempts\", 0)\n",
    "    cycles_loop = state.get(\"val_cycles\", 0)\n",
    "    \n",
    "    # Skip processing if the output files already exist\n",
    "    if os.path.exists(output_model_path) and os.path.exists(metadata_path) and attempts == 0 and cycles_loop == 0:\n",
    "        print(f\"Skipping {file_name} as output files already exist.\")\n",
    "\n",
    "        val_cycles_value = state.get(\"val_cycles\", 0) + 3\n",
    "        state[\"val_cycles\"] = val_cycles_value\n",
    "        \n",
    "        val_attemps_value = state.get(\"validation_attempts\", 0) + 3\n",
    "        state[\"validation_attempts\"] = val_attemps_value\n",
    "        \n",
    "        state[\"model_status\"] = \"skipped\"\n",
    "        return state\n",
    "    # else if \n",
    "\n",
    "    # Load the model file description\n",
    "    input_file_path = os.path.join(base_model_path, file_name)\n",
    "    model_description = load_file_content(input_file_path)\n",
    "    \n",
    "    # Load static content files\n",
    "    metamodel_content = load_file_content(metamodel_path)\n",
    "    example_model_content = load_file_content(example_model_path)\n",
    "    \n",
    "    # Define system prompt for model generation\n",
    "    system_prompt = (\n",
    "        \"You are an expert in Model-Driven Engineering and EMF (Eclipse Modeling Framework). \"\n",
    "        \"Your task is to generate a valid model instance that conforms to a given Ecore metamodel. \"\n",
    "        \"You will receive:\\n\"\n",
    "        \"- A context extracted using Retrieval-Augmented Generation (RAG) to provide relevant background information.\\n\\n\"\n",
    "        \"- A metamodel definition (in Ecore format).\\n\"\n",
    "        \"- An example model that demonstrates the structure and syntax of a valid instance.\\n\"\n",
    "        \"- A text-based description of the desired model (in natural language).\\n\"\n",
    "        \"You must return a syntactically correct and complete model file that adheres to the metamodel structure,\"\n",
    "        \"accurately represents the input description, and follows the style of the example provided.\"\n",
    "        #\"The output should be in plain text, without any explanations, comments, or extra formatting.\"\n",
    "        \"**Do not** add comments, backticks (```), or markdown formatting like ```xml. \"\n",
    "        \"The output must be pure XML text, starting directly with the XML declaration `<?xml version=\\\"1.0\\\" ...>` and ending with the last closing tag. \"\n",
    "        \"Do not prepend or append anything outside the model content.\"\n",
    "        \"Only return the model file content.\"\n",
    "    )\n",
    "    \n",
    "    chat_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"user\", (\n",
    "                f\"Context:\\n{context_llm}\\n\\n\"\n",
    "                \"This is the metamodel:\\n\\n{metamodel_content}\\n\\n\"\n",
    "                \"(Input example) This is an example model based on the given metamodel:\\n\\n{example_model_content}\\n\\n\"\n",
    "                \"(Input) Generate the model file for the following text description:\\n\\n{model_description}\\n\\n\"\n",
    "                \"Do not add comments or extra quotation marks at the beginning and end of the model file.\"\n",
    "            )),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Invoke the LLM chain for model generation\n",
    "    start_time_llm = time.time()\n",
    "    response_chain = chat_prompt | llm_LangChain\n",
    "    # print(f\"Final Prompt:{response_chain}\")\n",
    "    result = response_chain.invoke({\n",
    "        \"context\": context_llm,\n",
    "        \"metamodel_content\": metamodel_content,\n",
    "        \"example_model_content\": example_model_content,\n",
    "        \"model_description\": model_description,\n",
    "    })\n",
    "    print(f\"Final Results:{result}\")\n",
    "    end_time_llm = time.time()\n",
    "    execution_time = end_time_llm - start_time_llm\n",
    "    \n",
    "    # Extract the model from the result\n",
    "    if LLM_TYPE != 'Ollama':\n",
    "        model_output = result.content.strip()\n",
    "    else:\n",
    "        # model_output = result.strip()\n",
    "        # Extract the model from the result\n",
    "        if hasattr(result, \"content\"):\n",
    "            model_output = result.content.strip()\n",
    "        else:\n",
    "            model_output = str(result).strip()\n",
    "    \n",
    "    # Build metadata for the response\n",
    "    if LLM_TYPE != 'Ollama':\n",
    "        metadata = {\n",
    "            \"response_length\": len(model_output),\n",
    "            \"execution_time\": execution_time,\n",
    "            \"temperature\": temperature,\n",
    "            \"usage\": result.usage_metadata,\n",
    "            \"price_usd\": result.usage_metadata.get(\"input_tokens\", 0) * PRICE_PER_INPUT_TOKEN +\n",
    "                         result.usage_metadata.get(\"output_tokens\", 0) * PRICE_PER_OUTPUT_TOKEN,\n",
    "            \"model_name\": model_name\n",
    "        }\n",
    "    else:\n",
    "        metadata = {\n",
    "            \"response_length\": len(model_output),\n",
    "            \"execution_time\": execution_time,\n",
    "            \"temperature\": temperature,\n",
    "            \"model_name\": model_name\n",
    "        }\n",
    "\n",
    "    # Check if metadata file already exists\n",
    "    \"\"\"\n",
    "    if os.path.exists(metadata_path):\n",
    "        try:\n",
    "            with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                existing_metadata = json.load(f)\n",
    "            print(f\"[INFO] Loaded existing metadata from {metadata_path}\")\n",
    "    \n",
    "            # Increment numeric fields if they exist\n",
    "            for key in [\"execution_time\", \"response_length\", \"price_usd\"]:\n",
    "                if key in metadata and key in existing_metadata:\n",
    "                    if isinstance(metadata[key], (int, float)) and isinstance(existing_metadata[key], (int, float)):\n",
    "                        metadata[key] += existing_metadata[key]\n",
    "    \n",
    "            # Optionally merge nested 'usage' dictionaries if present\n",
    "            if \"usage\" in metadata and \"usage\" in existing_metadata:\n",
    "                for token_key in [\"input_tokens\", \"output_tokens\", \"total_tokens\"]:\n",
    "                    if token_key in metadata[\"usage\"] and token_key in existing_metadata[\"usage\"]:\n",
    "                        metadata[\"usage\"][token_key] += existing_metadata[\"usage\"][token_key]\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Could not read or parse existing metadata: {e}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save the model and metadata to output files\n",
    "    save_to_file(output_model_path, model_output)\n",
    "    save_metadata(metadata_path, metadata)\n",
    "    \n",
    "    print(f\"Processed: {file_name}\")\n",
    "    print(f\"Model saved to: {output_model_path}\")\n",
    "    print(f\"Metadata saved to: {metadata_path}\")\n",
    "\n",
    "    val_cycles_value = state.get(\"val_cycles\", 0) + 1\n",
    "    state[\"val_cycles\"] = val_cycles_value\n",
    "    \n",
    "    state[\"model_status\"] = \"processed\"\n",
    "    state[\"model_out\"] = model_output\n",
    "    state[\"metadata\"] = metadata\n",
    "    state[\"validation_attempts\"] = 0\n",
    "    return state\n",
    "\n",
    "# Node: model Generation – generate model and JSON metadata from a single file.\n",
    "# TODO: change this function to model generation and feedback with Eclipse EMF\n",
    "@profile_node\n",
    "def model_validation_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    file_name = state[\"file_name\"]\n",
    "    \n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "    # Load the generated model file description\n",
    "    output_model_path_val = os.path.join(base_output_dir, file_name)\n",
    "    output_model_path_val_abs = os.path.abspath(output_model_path_val)\n",
    "\n",
    "    # Connect to the running Java EMF Validator\n",
    "    gateway = JavaGateway()\n",
    "    validator = gateway.entry_point\n",
    "\n",
    "    # Paths to the BPMN Model and Ecore Metamodel\n",
    "    file_extension = \"bpmn\"  # \"bpmn\", \"caex\", etc.\n",
    "\n",
    "    # Model Validation\n",
    "    metamodel_path_val_abs = os.path.abspath(metamodel_path)\n",
    "\n",
    "    print(\"[DEBUG] Validating with:\")\n",
    "    print(\" - Model:\", output_model_path_val_abs)\n",
    "    print(\" - Metamodel:\", metamodel_path_val_abs)\n",
    "    \n",
    "    result = validator.validateModel(output_model_path_val_abs, metamodel_path_val_abs, file_extension)\n",
    "    print(result)\n",
    "    \n",
    "    state[\"model_val\"] = True\n",
    "    return state\n",
    "    \"\"\"\n",
    "    val_cycles = state.get(\"val_cycles\", 0)\n",
    "    validation_attempts = state.get(\"validation_attempts\", 0)\n",
    "\n",
    "    file_name = state[\"file_name\"]\n",
    "    # name_val = \"refined\"\n",
    "\n",
    "    # Split the filename into name and extension\n",
    "    file_base, file_ext = os.path.splitext(file_name)\n",
    "    print(f\"file_base: {file_base}, file_ext: {file_ext}\")\n",
    "\n",
    "    # Append the index to the base name\n",
    "    file_name_metadata = f\"{file_base}_{val_cycles}_{validation_attempts}{file_ext}\"\n",
    "    print(f\"file_name_metadata: {file_name_metadata}\")\n",
    "\n",
    "    os.makedirs(base_output_dir, exist_ok=True)\n",
    "    output_model_path_val = os.path.join(base_output_dir, file_name)\n",
    "    output_model_path_val_abs = os.path.abspath(output_model_path_val)\n",
    "    metamodel_path_val_abs = os.path.abspath(metamodel_path)\n",
    "\n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(base_output_json_dir, exist_ok=True)\n",
    "    metadata_path_val = os.path.join(base_output_json_dir, file_name_metadata)\n",
    "\n",
    "    # Check if metadata file already exists\n",
    "    \"\"\"\n",
    "    if os.path.exists(metadata_path_val):\n",
    "        \n",
    "        # Flag model as corrected (will be revalidated on next iteration)\n",
    "        val_cycles_value = state.get(\"val_cycles\", 0) + 3\n",
    "        state[\"val_cycles\"] = val_cycles_value\n",
    "        \n",
    "        val_attemps_value = state.get(\"validation_attempts\", 0) + 3\n",
    "        state[\"validation_attempts\"] = val_attemps_value\n",
    "        \n",
    "        state[\"model_val\"] = False  # For now mark as not valid (you can revalidate if desired)\n",
    "        return state  \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "        try:\n",
    "            with open(metadata_path_val, \"r\", encoding=\"utf-8\") as f:\n",
    "                existing_metadata = json.load(f)\n",
    "            print(f\"[INFO] Loaded existing metadata from {metadata_path_val}\")\n",
    "    \n",
    "            # Increment numeric fields if they exist\n",
    "            for key in [\"execution_time\", \"response_length\", \"price_usd\"]:\n",
    "                if key in metadata and key in existing_metadata:\n",
    "                    if isinstance(metadata[key], (int, float)) and isinstance(existing_metadata[key], (int, float)):\n",
    "                        metadata[key] += existing_metadata[key]\n",
    "    \n",
    "            # Optionally merge nested 'usage' dictionaries if present\n",
    "            if \"usage\" in metadata and \"usage\" in existing_metadata:\n",
    "                for token_key in [\"input_tokens\", \"output_tokens\", \"total_tokens\"]:\n",
    "                    if token_key in metadata[\"usage\"] and token_key in existing_metadata[\"usage\"]:\n",
    "                        metadata[\"usage\"][token_key] += existing_metadata[\"usage\"][token_key]\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Could not read or parse existing metadata: {e}\")\n",
    "    \"\"\"\n",
    "\n",
    "    # Connect to the Java EMF Validator\n",
    "    gateway = JavaGateway()\n",
    "    validator = gateway.entry_point\n",
    "    file_extension = \"bpmn\"\n",
    "\n",
    "    print(\"[DEBUG] Validating with:\")\n",
    "    print(\" - Model:\", output_model_path_val_abs)\n",
    "    print(\" - Metamodel:\", metamodel_path_val_abs)\n",
    "\n",
    "    validation_result = validator.validateModel(output_model_path_val_abs, metamodel_path_val_abs, file_extension)\n",
    "    print(\"[VALIDATION RESULT]\", validation_result)\n",
    "\n",
    "    # CASE 1: VALID MODEL\n",
    "    if \"Validation successful\" in validation_result:\n",
    "        print(\"Model is valid.\")\n",
    "        state[\"model_val\"] = True\n",
    "        state[\"next_step\"] = \"validated\"\n",
    "        return state\n",
    "    else:\n",
    "        attempts = state.get(\"validation_attempts\", 0) + 1\n",
    "        state[\"validation_attempts\"] = attempts\n",
    "\n",
    "        if attempts >= 3:\n",
    "            if val_cycles >= 2:\n",
    "                print(f\"[ERROR] Model is still invalid after {val_cycles} generation-validation cycles. Stopping.\")\n",
    "                state[\"next_step\"] = \"not_validated\"\n",
    "                return state\n",
    "            else:\n",
    "                print(f\"[INFO] Model invalid. Will regenerate. Cycle: {val_cycles}, Attempts: {validation_attempts}\")\n",
    "                state[\"next_step\"] = \"regenerate\"\n",
    "        else:\n",
    "            state[\"next_step\"] = \"continue\"\n",
    "        state[\"model_val\"] = False\n",
    "\n",
    "    # CASE 2: INVALID MODEL — Need to correct with LLM\n",
    "    print(\"Model is invalid. Sending to LLM for correction.\")\n",
    "\n",
    "    model_content = load_file_content(output_model_path_val_abs)\n",
    "    context_llm = state.get(\"context_llm\", \"\")\n",
    "    metamodel_content = load_file_content(metamodel_path)\n",
    "    example_model_content = load_file_content(example_model_path)\n",
    "\n",
    "    correction_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", (\n",
    "            \"You are a Model-Driven Engineering (MDE) and Eclipse Modeling Framework (EMF) expert. \"\n",
    "            \"A model file failed validation against an Ecore metamodel. Your job is to fix the model.\\n\\n\"\n",
    "            \"You will be given:\\n\"\n",
    "            \"- The validation error message.\\n\"\n",
    "            \"- The metamodel definition (Ecore).\\n\"\n",
    "            \"- An example valid model.\\n\"\n",
    "            \"- The model that failed.\\n\\n\"\n",
    "            \"Correct the model so that it becomes valid. Return only the corrected model, with no comments or explanations.\"\n",
    "            \"Do not add extra text or quotation marks at the beginning and end of the model file.\"\n",
    "            \"Let's think step by step\"\n",
    "        )),\n",
    "        (\"user\", (\n",
    "            f\"Validation error:\\n{validation_result}\\n\\n\"\n",
    "            f\"Metamodel:\\n{metamodel_content}\\n\\n\"\n",
    "            f\"Example model:\\n{example_model_content}\\n\\n\"\n",
    "            f\"Invalid model:\\n{model_content}\\n\\n\"\n",
    "            f\"Provide the corrected model:\"\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "    correction_chain = correction_prompt | llm_LangChain\n",
    "    # print(correction_chain)\n",
    "\n",
    "    metamodel_content_escaped = metamodel_content.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    example_model_content_escaped = example_model_content.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    model_content_escaped = model_content.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "    # Invoke the LLM chain for model refinement\n",
    "    start_time_llm = time.time()\n",
    "    result = correction_chain.invoke({\n",
    "        \"validation_result\": validation_result,\n",
    "        \"metamodel_content\": metamodel_content_escaped, # metamodel_content,\n",
    "        \"example_model_content\": example_model_content_escaped, # example_model_content,\n",
    "        \"model_content\": model_content_escaped, # model_content,\n",
    "    })\n",
    "    end_time_llm = time.time()\n",
    "    execution_time = end_time_llm - start_time_llm\n",
    "\n",
    "    corrected_model_partial = result.content.strip() \n",
    "\n",
    "    # corrected_model = corrected_model_partial.strip()\n",
    "    #if not corrected_model.startswith(\"<?xml\"):\n",
    "    #    corrected_model = \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\" + corrected_model\n",
    "\n",
    "    # Pulisce il modello rimuovendo righe con ```\n",
    "    lines = corrected_model_partial.strip().splitlines()\n",
    "    cleaned_lines = [line for line in lines if not line.strip().startswith(\"```\")]\n",
    "    corrected_model = \"\\n\".join(cleaned_lines).strip()\n",
    "\n",
    "    # Build metadata for the response\n",
    "    if LLM_TYPE != 'Ollama':\n",
    "        metadata = {\n",
    "            \"response_length\": len(corrected_model),\n",
    "            \"execution_time\": execution_time,\n",
    "            \"temperature\": temperature,\n",
    "            \"usage\": result.usage_metadata,\n",
    "            \"price_usd\": result.usage_metadata.get(\"input_tokens\", 0) * PRICE_PER_INPUT_TOKEN +\n",
    "                         result.usage_metadata.get(\"output_tokens\", 0) * PRICE_PER_OUTPUT_TOKEN,\n",
    "            \"model_name\": model_name\n",
    "        }\n",
    "    else:\n",
    "        metadata = {\n",
    "            \"response_length\": len(corrected_model),\n",
    "            \"execution_time\": execution_time,\n",
    "            \"temperature\": temperature,\n",
    "            \"model_name\": model_name\n",
    "        }\n",
    "        \n",
    "    # Save the corrected model (overwrite original file)\n",
    "    save_to_file(output_model_path_val_abs, corrected_model)\n",
    "    save_metadata(metadata_path_val, metadata)\n",
    "\n",
    "    print(f\"Processed: {file_name}\")\n",
    "    print(f\"Corrected model saved to: {output_model_path_val_abs}\")\n",
    "    print(f\"Metadata saved to: {metadata_path_val}\")\n",
    "\n",
    "    # Track number of validation attempts\n",
    "    \"\"\"\n",
    "    attempts = state.get(\"validation_attempts\", 0) + 1\n",
    "    state[\"validation_attempts\"] = attempts\n",
    "    \n",
    "    # If attempts exceed max, restart from model_generation\n",
    "    MAX_ATTEMPTS = 3\n",
    "    if attempts >= MAX_ATTEMPTS:\n",
    "        print(f\"[INFO] Max validation attempts ({MAX_ATTEMPTS}) reached. Re-generating model.\")\n",
    "        return \"regenerate\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flag model as corrected (will be revalidated on next iteration)\n",
    "    state[\"model_val\"] = False  # For now mark as not valid (you can revalidate if desired)\n",
    "    return state    \n",
    "\n",
    "def model_to_MSE_node(state: GraphState) -> GraphState:\n",
    "    val_cycles = state.get(\"val_cycles\", 0)\n",
    "    if val_cycles >= 2:\n",
    "        print(f\"[ERROR] Model is still invalid after {val_cycles} generation-validation cycles. Stopping.\")\n",
    "    else:\n",
    "        print(\"Model generated correctly!!!\")\n",
    "    return state\n",
    "\n",
    "###################################\n",
    "#       GRAPH WORKFLOW SETUP      #\n",
    "###################################\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# Initialize the state graph using our GraphState type\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "\n",
    "# Add the new node to the workflow\n",
    "workflow.add_node(\"generate_query\", generate_query_node)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)             # Merged generate node\n",
    "workflow.add_node(\"generate_web\", generate_web)\n",
    "workflow.add_node(\"transform_query\", transform_query) # Merged transform node\n",
    "workflow.add_node(\"transform_query_web\", transform_query_web)\n",
    "\n",
    "# Add evaluation nodes for vectorstore branch\n",
    "workflow.add_node(\"evaluate_rag_output\", evaluate_rag_output)\n",
    "workflow.add_node(\"evaluate_bert_score\", evaluate_bert_score)\n",
    "\n",
    "# Add evaluation nodes for web search branch\n",
    "workflow.add_node(\"evaluate_web_search_output\", evaluate_web_search_output)\n",
    "workflow.add_node(\"evaluate_web_bert_score\", evaluate_web_bert_score)\n",
    "\n",
    "workflow.add_node(\"cache_context\", cache_context_node)  # Caching node\n",
    "workflow.add_node(\"model_generation\", model_generation_node)\n",
    "workflow.add_node(\"model_validation\", model_validation_node)\n",
    "workflow.add_node(\"model_to_MSE\", model_to_MSE_node)\n",
    "\n",
    "\n",
    "def custom_parse_router_output(raw_output: str) -> RouteQuery:\n",
    "    try:\n",
    "        data = json.loads(raw_output)\n",
    "        # If the output contains \"datasource\", use it directly.\n",
    "        if \"datasource\" in data:\n",
    "            return RouteQuery(**data)\n",
    "        # Alternatively, if the output contains a \"tool\" key and it equals \"BPMN-Designer\",\n",
    "        # you might decide to map it to one of your expected values.\n",
    "        elif data.get(\"tool\") == \"BPMN-Designer\":\n",
    "            # Here you can choose what \"BPMN-Designer\" should map to.\n",
    "            # For example, if BPMN-Designer is related to vectorstore, then:\n",
    "            return RouteQuery(datasource=\"vectorstore\")\n",
    "        else:\n",
    "            raise ValueError(\"Output does not contain a valid routing decision.\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to parse router output: {e}\")\n",
    "\n",
    "# Starting node: route question decides between web_search and vectorstore (retrieve)\n",
    "def route_question(state: GraphState) -> str:\n",
    "    # If the flag is present, skip the routing and return a special key (\"skip\")\n",
    "    if state.get(\"skip_router\", False):\n",
    "        print(\"Skipping routing; moving directly to cache_context.\")\n",
    "        return \"skip\"\n",
    "        \n",
    "    print(\"--- ROUTE QUESTION ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question_val, \"topics_str\": topics_str})\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed = custom_parse_router_output(source)\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing router output:\", e)\n",
    "        # Fallback to a default value\n",
    "        parsed = RouteQuery(datasource=\"web_search\")\n",
    "    \"\"\"\n",
    "        \n",
    "    # Normalize the datasource value.\n",
    "    datasource = source.datasource.lower().strip()\n",
    "    if datasource == \"vectorstore\":\n",
    "        print(\"--- Routing to vectorstore ---\")\n",
    "        state[\"branch\"] = \"retrieve\"\n",
    "        return \"vectorstore\"\n",
    "    elif datasource == \"web_search\":\n",
    "        print(\"--- Routing to web search ---\")\n",
    "        state[\"branch\"] = \"web_search\"\n",
    "        return \"web_search\"\n",
    "    state[\"branch\"] = \"retrieve\"\n",
    "    return \"vectorstore\"\n",
    "\n",
    "# Add an edge from the START node to the new \"generate_query\" node\n",
    "workflow.add_edge(START, \"generate_query\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_query\",\n",
    "    route_question,\n",
    "    {\n",
    "        \"skip\": \"cache_context\", # If the flag is active, go directly to cache_context_node\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"vectorstore\": \"retrieve\",  # Key now matches the returned normalized value\n",
    "    },\n",
    ")\n",
    "\n",
    "# For the web search branch, send directly to generate.\n",
    "workflow.add_edge(\"web_search\", \"generate_web\")\n",
    "\n",
    "# For the retrieve branch, first go to grade_documents.\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# After grading, decide whether to generate or transform.\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    lambda state: decide_to_generate(state),\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_edge(\"transform_query_web\", \"web_search\")\n",
    "\n",
    "# After generate/generate_web, grade the generation.\n",
    "# If the generation is \"useful\", route to the caching node.\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        # \"not supported\": \"generate\",\n",
    "        \"useful\": \"evaluate_rag_output\",\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"evaluate_rag_output\", \"evaluate_bert_score\")\n",
    "workflow.add_edge(\"evaluate_bert_score\", \"cache_context\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_web\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        # \"not supported\": \"generate_web\",\n",
    "        \"useful\": \"evaluate_web_search_output\",\n",
    "        \"not useful\": \"transform_query_web\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"evaluate_web_search_output\", \"evaluate_web_bert_score\")\n",
    "workflow.add_edge(\"evaluate_web_bert_score\", \"cache_context\")\n",
    "\n",
    "# After caching, flow to model generation.\n",
    "workflow.add_edge(\"cache_context\", \"model_generation\")\n",
    "# workflow.add_edge(\"model_generation\", END)  # End the workflow after model validation\n",
    "workflow.add_edge(\"model_generation\", \"model_validation\")  # End the workflow after model validation\n",
    "\n",
    "# If model_validation sets model_val = False, redirect to model_generation\n",
    "\"\"\"\n",
    "workflow.add_conditional_edges(\n",
    "    \"model_validation\",\n",
    "    lambda state: \"validated\" if state.get(\"model_val\") else \"continue\",\n",
    "    {\n",
    "        \"validated\": \"model_to_MSE\",  \n",
    "        \"continue\": \"model_validation\"  \n",
    "    },\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"model_validation\",\n",
    "    lambda state: state.get(\"next_step\", \"continue\"),  # Usa direttamente il campo\n",
    "    {\n",
    "        \"validated\": \"model_to_MSE\",\n",
    "        \"continue\": \"model_validation\",\n",
    "        \"regenerate\": \"model_generation\",\n",
    "        \"not_validated\": \"model_to_MSE\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# workflow.add_edge(\"model_validation\", END)  # End the workflow after model generation\n",
    "workflow.add_edge(\"model_to_MSE\", END)\n",
    "\n",
    "# Compile the workflow graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# Optionally visualize the graph (requires additional dependencies)\n",
    "try:\n",
    "    from IPython.display import display, Markdown, Image\n",
    "    # Retrieve the graph and set its configuration\n",
    "    graph = app.get_graph()\n",
    "    graph.mermaid_config = {\"graph_direction\": \"TD\"}\n",
    "\n",
    "    # Generate the PNG image bytes from the graph\n",
    "    png_bytes = graph.draw_mermaid_png()\n",
    "\n",
    "    # Save the image to disk as 'graph.png'\n",
    "    with open(\"graph.png\", \"wb\") as f:\n",
    "        f.write(png_bytes)\n",
    "    print(\"The graph has been saved as 'graph.png'.\")\n",
    "    \n",
    "    display(Markdown(\"### LangGraph Visualization ###\"))\n",
    "    display(Image(graph.draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(\"Graph rendering failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce3090-1fee-4617-86c7-0317297d2df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#       model GENERATION LOOP     #\n",
    "###################################\n",
    "\n",
    "# Global CodeCarbon tracker for the entire application\n",
    "global_cc_tracker = EmissionsTracker(\n",
    "    project_name=\"global_app\",\n",
    "    measure_power_secs=1,\n",
    "    output_dir=CODECARBON_FOLDER,\n",
    "    allow_multiple_runs=True\n",
    "    # api_call_interval=4,\n",
    "    # experiment_id=experiment_id,\n",
    "    # save_to_api=True\n",
    "    # log_to_api=True                     # Enable logging to the CodeCarbon online dashboard\n",
    "    # api_key=codecarbon_api_key,          # Provide your CodeCarbon API key here\n",
    "    # api_url=\"https://api.codecarbon.io\"   # (Optional) Specify the API endpoint if different from the default\n",
    ")\n",
    "global_cc_tracker.start()\n",
    "\n",
    "# Reset the overall summary for CodeCarbon per file\n",
    "cc_global_summary = []\n",
    "\n",
    "# List to collect summary records for each file (for final summary CSV)\n",
    "summary_records = []\n",
    "\n",
    "app_start_time = time.time()\n",
    "\n",
    "# For each file, add file name and the query to the state.\n",
    "# The cache_context node in the workflow will ensure the refined context is present.\n",
    "input_files = [file_name for file_name in os.listdir(base_model_path) if file_name.endswith(\".bpmn\")]\n",
    "\n",
    "for file_name in input_files:\n",
    "    # Record start time for this file\n",
    "    print(f\"Start Generating Model for {file_name}\")\n",
    "    file_start = time.time()\n",
    "    \n",
    "    # Record the starting index of the global profiling_records list\n",
    "    start_index = len(profiling_records)\n",
    "\n",
    "    # Start a file-level CodeCarbon tracker\n",
    "    file_cc_tracker = EmissionsTracker(\n",
    "        project_name=\"global_file_\" + file_name,\n",
    "        measure_power_secs=1,\n",
    "        output_dir=CODECARBON_FOLDER,\n",
    "        allow_multiple_runs=True,\n",
    "        api_call_interval=4\n",
    "        # experiment_id=experiment_id,\n",
    "        # save_to_api=True\n",
    "        # log_to_api=True                     # Enable logging to the CodeCarbon online dashboard for each file\n",
    "        # api_key=codecarbon_api_key,          # Provide your CodeCarbon API key here\n",
    "        # api_url=\"https://api.codecarbon.io\"   # (Optional) Specify the API endpoint if different from the default\n",
    "    )\n",
    "    file_cc_tracker.start()\n",
    "    \n",
    "    # Reset the per-node CodeCarbon metrics for this file\n",
    "    cc_metrics_for_file = []\n",
    "    \n",
    "    state = GraphState()\n",
    "    state[\"file_name\"] = file_name         # Provide file name for model generation\n",
    "    # state[\"question\"] = question           # The query generated from the metamodel\n",
    "    # Run the workflow using stream() and take the final output state\n",
    "    result_state = list(app.stream(state, config={\"recursion_limit\": 25}))[-1]\n",
    "    \n",
    "    # Record end time for this file and calculate overall time\n",
    "    file_end = time.time()\n",
    "    overall_time = file_end - file_start\n",
    "\n",
    "    # Stop the file-level CodeCarbon tracker and get global metrics for the file\n",
    "    file_emissions = file_cc_tracker.stop()\n",
    "    # Try to get detailed metrics if available\n",
    "    if hasattr(file_cc_tracker, \"_final_emissions_data\"):\n",
    "        file_metrics = file_cc_tracker._final_emissions_data\n",
    "    else:\n",
    "        file_metrics = {\"total_emissions\": file_emissions}\n",
    "\n",
    "    # Extract profiling records corresponding to this file\n",
    "    file_records = profiling_records[start_index:].copy()\n",
    "    # Append an additional record for the overall file execution time\n",
    "    file_records.append({\"node\": f\"FILE_{file_name}\", \"execution_time\": overall_time})\n",
    "    \n",
    "    # Save the profiling data for this file in a dedicated CSV file if it doesn't already exist\n",
    "    csv_file_path = os.path.join(PROFILING_FOLDER, f\"profiling_{file_name}.csv\")\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        with open(csv_file_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "            fieldnames = [\"node\", \"execution_time\"]\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for record in file_records:\n",
    "                writer.writerow(record)\n",
    "        print(f\"Profiling data for {file_name} saved to {csv_file_path}\")\n",
    "    else:\n",
    "        print(f\"Profiling file {csv_file_path} already exists. Skipping save.\")\n",
    "    \n",
    "    # Add a summary record for this file\n",
    "    summary_records.append({\"file_name\": file_name, \"execution_time\": overall_time})\n",
    "\n",
    "    ############ CODE CARBON ##############\n",
    "    # Save per-node CodeCarbon metrics along with file-level metrics into a dedicated CSV file,\n",
    "    # with file name starting with \"codecarbon_\"\n",
    "    cc_csv_file = os.path.join(CODECARBON_FOLDER, f\"codecarbon_{file_name}.csv\")\n",
    "    if not os.path.exists(cc_csv_file):\n",
    "        # Prepare a list of rows: one row per node metric, plus one row for overall file metrics.\n",
    "        # We merge the per-node metrics (from cc_metrics_for_file) into a list.\n",
    "        # Note: Each metric row is a dictionary. We also add a row for the file global metrics.\n",
    "        rows = []\n",
    "        for record in cc_metrics_for_file:\n",
    "            # record already contains \"node\" and various CodeCarbon metrics\n",
    "            rows.append(record)\n",
    "        # Append a row for overall file CodeCarbon metrics:\n",
    "        overall_record = {\"node\": f\"FILE_{file_name}\"}\n",
    "        overall_record.update(file_metrics)\n",
    "        rows.append(overall_record)\n",
    "        \n",
    "        # Determine all possible keys across all rows\n",
    "        all_keys = set()\n",
    "        for r in rows:\n",
    "            all_keys.update(r.keys())\n",
    "        all_keys = list(all_keys)\n",
    "        \n",
    "        with open(cc_csv_file, mode=\"w\", newline=\"\") as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=all_keys)\n",
    "            writer.writeheader()\n",
    "            for r in rows:\n",
    "                writer.writerow(r)\n",
    "        print(f\"CodeCarbon metrics for {file_name} saved to {cc_csv_file}\")\n",
    "    else:\n",
    "        print(f\"CodeCarbon file {cc_csv_file} already exists. Skipping save.\")\n",
    "    \n",
    "    # Append summary record for this file (global CodeCarbon metrics)\n",
    "    cc_global_summary.append({\"file_name\": file_name, **file_metrics})\n",
    "\n",
    "    ############## RAG EVALUATION ################\n",
    "    \n",
    "    skip_router_value = result_state[\"model_to_MSE\"][\"skip_router\"]\n",
    "    # skip_router_value = result_state.get(\"skip_router\", False)\n",
    "    print(\"skip_router:\", skip_router_value)\n",
    "\n",
    "    evaluation_metrics = result_state[\"model_to_MSE\"].get(\"evaluation_metrics\")\n",
    "    # evaluation_metrics = result_state.get(\"evaluation_metrics\", {})\n",
    "    print(\"evaluation_metrics:\", evaluation_metrics)\n",
    "\n",
    "    bert_score_metrics = result_state[\"model_to_MSE\"].get(\"bert_score\")\n",
    "    # bert_score_metrics = result_state.get(\"bert_score\", {})\n",
    "    print(\"bert_score_metrics:\", bert_score_metrics)\n",
    "\n",
    "    web_bert_score_metrics = result_state[\"model_to_MSE\"].get(\"web_bert_score\")\n",
    "    # web_bert_score_metrics = result_state.get(\"web_bert_score\", {})\n",
    "    print(\"evaluation_metrics:\", web_bert_score_metrics)\n",
    "\n",
    "    if not skip_router_value:\n",
    "        # Save evaluation results to CSV for this file (if evaluation metrics exist)\n",
    "        evaluation_data = {\"file_name\": file_name}\n",
    "        \n",
    "        # Use get() with a default empty dict to ensure we update with available metrics\n",
    "        evaluation_data.update(result_state[\"model_to_MSE\"].get(\"evaluation_metrics\", {}))\n",
    "        evaluation_data.update(result_state[\"model_to_MSE\"].get(\"bert_score\", {}))\n",
    "        evaluation_data.update(result_state[\"model_to_MSE\"].get(\"web_bert_score\", {}))\n",
    "        \n",
    "        eval_csv_file = os.path.join(EVALUATION_FOLDER, f\"evaluation_{file_name}.csv\")\n",
    "        with open(eval_csv_file, \"w\", newline=\"\") as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=evaluation_data.keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerow(evaluation_data)\n",
    "        print(f\"Evaluation results for {file_name} saved to {eval_csv_file}\")\n",
    "    \n",
    "    print(f\"Model generation result for {file_name}: {result_state.get('model_status', 'unknown')}\")\n",
    "\n",
    "# Record end time of the entire application and calculate total time\n",
    "app_end_time = time.time()\n",
    "total_app_time = app_end_time - app_start_time\n",
    "summary_records.append({\"file_name\": \"TOTAL_APP\", \"execution_time\": total_app_time})\n",
    "\n",
    "global_summary = global_cc_tracker.stop()\n",
    "if hasattr(global_cc_tracker, \"_final_emissions_data\"):\n",
    "    global_metrics = global_cc_tracker._final_emissions_data\n",
    "else:\n",
    "    global_metrics = {\"total_emissions\": global_summary}\n",
    "cc_global_summary.append({\"file_name\": \"TOTAL_APP\", **global_metrics})\n",
    "print(\"END model GENERATION PROCESS!!!\")\n",
    "\n",
    "# Save the final summary CSV with overall times per file if it doesn't already exist\n",
    "final_csv_file = os.path.join(PROFILING_FOLDER, \"profiling_summary.csv\")\n",
    "if not os.path.exists(final_csv_file):\n",
    "    with open(final_csv_file, mode=\"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"file_name\", \"execution_time\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for record in summary_records:\n",
    "            writer.writerow(record)\n",
    "    print(f\"Summary profiling data saved to {final_csv_file}\")\n",
    "else:\n",
    "    print(f\"Summary profiling CSV {final_csv_file} already exists. Skipping save.\")\n",
    "\n",
    "# Save the global CodeCarbon summary into a CSV file\n",
    "global_csv_file = os.path.join(CODECARBON_FOLDER, \"codecarbon_summary.csv\")\n",
    "if not os.path.exists(global_csv_file):\n",
    "    fieldnames = set()\n",
    "    for record in cc_global_summary:\n",
    "        fieldnames.update(record.keys())\n",
    "    fieldnames = list(fieldnames)\n",
    "    with open(global_csv_file, mode=\"w\", newline=\"\") as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for record in cc_global_summary:\n",
    "            writer.writerow(record)\n",
    "    print(f\"Global CodeCarbon summary saved to {global_csv_file}\")\n",
    "else:\n",
    "    print(f\"Global CodeCarbon summary file {global_csv_file} already exists. Skipping save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c734b01-6781-46ad-8b8e-26b36f37812a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
