{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd3e468",
   "metadata": {},
   "source": [
    "# CAEX Few Shot in Context Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f4909",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "41215d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_mistralai.chat_models.ChatMistralAI'>\n",
      "content='event BehaviorSpecification ADD\\nevent BehaviorSpecification name SET\\nevent BehaviorSpecification nodes ADD\\nevent Stimulus ADD\\nevent Stimulus name SET\\nevent Stimulus ports ADD\\nevent Port name SET\\nevent Port pChannels ADD\\nevent Channel ADD\\nevent Channel name SET\\nevent Channel pFrom SET\\nevent Channel pTo SET\\nevent Channel queueSize SET\\nevent Channel rendezVous SET\\nevent Channel message ADD\\nevent Message ADD\\nevent Message name SET\\nevent Message entry ADD\\nevent Entry ADD\\nevent Entry name SET\\nevent Entry type SET\\nevent BehaviorSpecification nodes ADD\\nevent Display ADD\\nevent Display name SET\\nevent Display ports ADD\\nevent Port name SET\\nevent Port pChannels ADD\\nevent Channel ADD\\nevent Channel pFrom SET\\nevent Port pChannels ADD\\nevent Channel ADD\\nevent Channel pFrom SET\\nevent Channel pTo SET\\nevent BehaviorSpecification nodes ADD\\nevent StructuredNode ADD\\nevent StructuredNode name SET\\nevent StructuredNode nChannels ADD\\nevent Channel ADD\\nevent Channel name SET\\nevent Channel nFrom SET\\nevent Channel nTo SET\\nevent Channel queueSize SET\\nevent Channel rendezVous SET\\nevent Channel message ADD\\nevent Message ADD\\nevent Message name SET\\nevent Message entry ADD\\nevent Entry ADD\\nevent Entry name SET\\nevent Entry type SET\\nevent StructuredNode nChannels ADD\\nevent Channel ADD\\nevent Channel name SET\\nevent Channel nFrom SET\\nevent Channel nTo SET\\nevent Channel queueSize SET\\nevent Channel rendezVous SET\\nevent Channel message ADD\\nevent Message ADD\\nevent Message name SET\\nevent Message entry ADD\\nevent Entry ADD\\nevent Entry name SET\\nevent Entry type SET\\nevent StructuredNode nChannels ADD\\nevent Channel ADD\\nevent Channel name SET\\nevent Channel nFrom SET\\nevent Channel nTo SET\\nevent Channel queueSize SET\\nevent Channel rendezVous SET\\nevent Channel message ADD\\nevent Message ADD\\nevent Message name SET\\nevent Message entry ADD\\nevent Entry ADD\\nevent Entry name SET\\nevent Entry type SET\\nevent StructuredNode nChannels ADD\\nevent Channel ADD\\nevent Channel name SET\\nevent Channel nFrom SET\\nevent Channel nTo SET\\nevent Channel queueSize SET\\nevent Channel rendezVous SET\\nevent Channel message ADD\\nevent Message ADD\\nevent Message name SET\\nevent Message entry ADD\\nevent Entry ADD\\nevent Entry name SET\\nevent Entry type SET\\nevent StructuredNode ports ADD\\nevent Port ADD\\nevent Port name SET\\nevent Port portExtension SET\\nevent StructuredNode ports ADD\\nevent Port ADD\\nevent Port name SET\\nevent StructuredNode processes ADD\\nevent Process ADD\\nevent Process name SET\\nevent StructuredNode processes ADD\\nevent Process ADD\\nevent Process name SET\\nevent StructuredNode processes ADD\\nevent Process ADD\\nevent Process name SET\\nevent StructuredNode processes ADD\\nevent Process ADD\\nevent Process name SET\\nevent Process processExtension SET' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 7038, 'total_tokens': 7664, 'completion_tokens': 626}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run-3425985d-b29b-4cfd-8eff-e2045dd5d2af-0' usage_metadata={'input_tokens': 7038, 'output_tokens': 626, 'total_tokens': 7664}\n",
      "Processed: 2024-02-13 16.44 00%20-%20DigitalCam%20Nominal-representations.aird.hepsy\n",
      "XES saved to: D2-HEPSYCODE/XES-MORGAN-LLM-mistral-large-latest\\2024-02-13 16.44 00%20-%20DigitalCam%20Nominal-representations.aird.xes\n",
      "Metadata saved to: D2-HEPSYCODE/XES-MORGAN-LLM-mistral-large-latest\\2024-02-13 16.44 00%20-%20DigitalCam%20Nominal-representations.aird.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.llms import Ollama\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "# Load configuration from JSON (mistral, openai, ollama, anthropic, google)\n",
    "CONFIG_FILE = \"config/llm_config_mistral.json\"\n",
    "MODELS_FILE = \"config/llm_models.json\"\n",
    "\n",
    "def load_config(config_file):\n",
    "    try:\n",
    "        with open(config_file, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Configuration file {config_file} not found.\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Load configurations\n",
    "config = load_config(CONFIG_FILE)\n",
    "models_config = load_config(MODELS_FILE)\n",
    "\n",
    "# Extract parameters from configuration\n",
    "LLM = config.get(\"llm\")\n",
    "if not LLM:\n",
    "    raise ValueError(\"LLM name must be specified in the configuration file.\")\n",
    "\n",
    "PRICE_PER_INPUT_TOKEN = config.get(\"price_per_input_token\")\n",
    "PRICE_PER_OUTPUT_TOKEN = config.get(\"price_per_output_token\")\n",
    "temperature = config.get(\"temperature\")\n",
    "max_retries = config.get(\"max_retries\")\n",
    "api_key = config.get(\"api_keys\", {}).get(LLM.lower(), None)\n",
    "base_url = config.get(\"base_url\")\n",
    "\n",
    "# Get model configuration\n",
    "LLM_TYPE = 'Other'\n",
    "llm_config = models_config.get(LLM, None)\n",
    "if llm_config and LLM_TYPE != 'Ollama':\n",
    "    # Update parameters dynamically\n",
    "    llm_params = llm_config.get(\"params\", {})\n",
    "    llm_params[\"temperature\"] = temperature\n",
    "    llm_params[\"max_retries\"] = max_retries\n",
    "    llm_params[\"api_key\"] = api_key\n",
    "    llm_params[\"base_url\"] = base_url\n",
    "\n",
    "    # Initialize LLM\n",
    "    llm_class = eval(llm_config[\"class\"])\n",
    "    print(llm_class)\n",
    "    # print(llm_params)\n",
    "    llm_LangChain = llm_class(**llm_params)\n",
    "    model_name = LLM  # Use LLM directly as the model name\n",
    "elif LLM_TYPE == 'Ollama':\n",
    "    llm_params = llm_config.get(\"params\", {})\n",
    "    llm_params[\"temperature\"] = temperature\n",
    "    llm_params[\"base_url\"] = base_url\n",
    "\n",
    "    # Initialize LLM\n",
    "    llm_class = eval(llm_config[\"class\"])\n",
    "    print(llm_class)\n",
    "    # print(llm_params)\n",
    "    llm_LangChain = llm_class(**llm_params)\n",
    "    model_name = LLM  # Use LLM directly as the model name\n",
    "else:\n",
    "    raise ValueError(f\"Model configuration for '{LLM}' not found in {MODELS_FILE}.\")\n",
    "\n",
    "# File paths\n",
    "metamodel_path = \"../../01-02-03_MSE/CAEX/CAEX-Metamodels/CAEX30.ecore\"\n",
    "example_model_path = \"../../01-02-03_MSE/CAEX/CAEX-Models/D1/CAEX/2023-02-10 10.09 Test-representations.airdM.caex\"\n",
    "example_xes_trace_path = \"../../04_Trace_Parser/D1_HEPSYCODE/XES-MORGAN/2024-02-14 18.30 13%20-%20FIRFIRGCD_HPV-representations.aird.xes\"\n",
    "\n",
    "# Base paths\n",
    "base_model_path = \"../../01-02-03_MSE/HEPSYCODE/HEPSYCODE-Models/D1/HEPSY/\"\n",
    "base_output_dir = f\"D2-HEPSYCODE/XES-MORGAN-LLM-{model_name.lower()}\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "# Utility functions\n",
    "def load_file_content(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found.\")\n",
    "        return \"\"\n",
    "\n",
    "def save_to_file(file_path, content):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def save_metadata(file_path, metadata):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(metadata, file, indent=4)\n",
    "\n",
    "# Few-shot context\n",
    "context = (\n",
    "    \"We have developed an Electronic Design Automation tool for designing embedded systems called HEPSYCODE. \"\n",
    "    \"The tool is built upon Eclipse Ecore metamodels and utilizes Sirius features. \"\n",
    "    \"The metamodel is designed to model algorithms, functionalities, and applications as a process network.\"\n",
    ")\n",
    "\n",
    "# Chat few shot prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",  \"You are an expert in embedded systems design with deep knowledge of using Electronic Design\"\n",
    "            \"Automation (EDA) tools at the system level. Your role is to assist designers and engineers\" \n",
    "            \"in developing system-level models for complex embedded applications.\\n\"\n",
    "            \"You must provide precise and detailed solutions following best practices in embedded systems engineering.\\n\"\n",
    "            \"You are expected to explain and implement concepts such as system-level simulation,\"\n",
    "            \"hardware/software co-design, design space exploration, and event-driven or cycle-accurate modeling.\\n\"\n",
    "            \"You are proficient with tools like SystemC, MATLAB/Simulink, and similar frameworks, and know\"\n",
    "            \"how to use them to describe functional behavior, system architecture, and constraints.\\n\"\n",
    "            \"Your approach should focus on verification, validation, and design optimization, \"\n",
    "            \"considering parameters like performance, power consumption, cost, and time-to-market.\\n\"\n",
    "            \"You must be clear and concise, capable of providing practical examples and guidance on \"\n",
    "            \"solving specific problems or optimizing the design workflow.\\n\"\n",
    "            \"Ensure your communication is professional and tailored to a technical audience.\\n\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Context:\\n{context}\\n\\n\"\n",
    "            \"This is the HEPSY metamodel:\\n\\n{hepsycode_metamodel}\\n\\n\"\n",
    "            \"This is an example model based on HEPSYCODE metamodel:\\n\\n{hepsycode_example_model}\\n\\n\"\n",
    "            \"This is a XES trace file representing the modeling step:\\n\\n{xes_example_trace}\\n\\n\"\n",
    "            \"Generate an XES trace file for the following model:\\n\\n{hepsycode_model}\\n\\nOutput:\\n\"\n",
    "            \"event StructuredNode processes ADD\\n\"\n",
    "            \"event Process name SET\\n\"\n",
    "            \"event Port portExtension SET\\n\"\n",
    "            \"event Port portExtension SET\\n\"\n",
    "            \"event StructuredNode nChannels ADD\\n\"\n",
    "            \"event Channel nFrom SET\\n\"\n",
    "            \"event Channel nTo SET\\n\"\n",
    "            \"Write only the events, not other information in output\"\n",
    "            \"Do not add comments\"\n",
    "            \"Do not add double quotation marks at the beginning and end of the  XES trace\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Process all .hepsy files\n",
    "input_files = [file_name for file_name in os.listdir(base_model_path) if file_name.endswith(\".hepsy\")]\n",
    "\n",
    "hepsycode_metamodel = load_file_content(metamodel_path)\n",
    "hepsycode_example_model = load_file_content(example_model_path)\n",
    "xes_example_trace = load_file_content(example_xes_trace_path)\n",
    "\n",
    "i = 0\n",
    "for file_name in input_files:\n",
    "    if i < 1:\n",
    "        i = i + 1\n",
    "        input_file_path = os.path.join(base_model_path, file_name)\n",
    "        hepsycode_model = load_file_content(input_file_path)\n",
    "\n",
    "        # Invoke the model\n",
    "        response = chat_prompt | llm_LangChain\n",
    "        result = response.invoke(\n",
    "            {\n",
    "                \"context\": context,\n",
    "                \"hepsycode_metamodel\": hepsycode_metamodel,\n",
    "                \"hepsycode_example_model\": hepsycode_example_model,\n",
    "                \"xes_example_trace\": xes_example_trace,\n",
    "                \"hepsycode_model\": hepsycode_model,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Extract and save results\n",
    "        output_xes_path = os.path.join(base_output_dir, file_name.replace(\".hepsy\", \".xes\"))\n",
    "        metadata_path = os.path.join(base_output_dir, file_name.replace(\".hepsy\", \".json\"))\n",
    "\n",
    "        if LLM_TYPE != 'Ollama':\n",
    "            xes_trace = result.content.strip()\n",
    "        else:\n",
    "            xes_trace = result.strip()\n",
    "        print(result)\n",
    "        \n",
    "        if LLM_TYPE != 'Ollama':\n",
    "            metadata = {\n",
    "                \"response_length\": len(xes_trace),\n",
    "                \"temperature\": temp,\n",
    "                \"usage\": result.usage_metadata,\n",
    "                \"price_usd\": result.usage_metadata.get(\"input_tokens\", 0) * PRICE_PER_INPUT_TOKEN + result.usage_metadata.get(\"output_tokens\", 0) * PRICE_PER_OUTPUT_TOKEN,\n",
    "                \"model_name\": model_name\n",
    "            }\n",
    "        else:\n",
    "            metadata = {\n",
    "                \"response_length\": len(xes_trace),\n",
    "                \"temperature\": temp,\n",
    "                \"model_name\": model_name\n",
    "            }\n",
    "\n",
    "        save_to_file(output_xes_path, xes_trace)\n",
    "        save_metadata(metadata_path, metadata)\n",
    "\n",
    "        print(f\"Processed: {file_name}\")\n",
    "        print(f\"XES saved to: {output_xes_path}\")\n",
    "        print(f\"Metadata saved to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e9337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
