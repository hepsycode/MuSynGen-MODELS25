{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ad24b88-fdd7-4aaa-8dd3-0bdb38ada905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature:  0.0\n",
      "Model:  mistral-large-latest\n",
      "Loaded 13 URLs from 'config/BASE_URL-FULL.csv'.\n",
      "The folder 'faiss' already exists.\n",
      "Loading existing FAISS index from disk...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntry:\\n    from IPython.display import display, Markdown, Image\\n    # Retrieve the graph and set its configuration\\n    graph = app.get_graph()\\n    graph.mermaid_config = {\"graph_direction\": \"TD\"}\\n\\n    # Generate the PNG image bytes from the graph\\n    png_bytes = graph.draw_mermaid_png()\\n\\n    # Save the image to disk as \\'graph.png\\'\\n    with open(\"graph.png\", \"wb\") as f:\\n        f.write(png_bytes)\\n    print(\"The graph has been saved as \\'graph.png\\'.\")\\n    \\n    display(Markdown(\"### LangGraph Visualization ###\"))\\n    display(Image(graph.draw_mermaid_png()))\\nexcept Exception as e:\\n    print(\"Graph rendering failed:\", e)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "###################################\n",
    "#           LIBRARIES             #\n",
    "###################################\n",
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import uuid\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "import textdistance\n",
    "import httpx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET  # For parsing the Ecore file\n",
    "\n",
    "from codecarbon import EmissionsTracker  # Import CodeCarbon\n",
    "\n",
    "# LangChain and related libraries\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.llms import Ollama\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.tools.base import Tool\n",
    "from typing import Callable\n",
    "\n",
    "from pytextdist.edit_distance import levenshtein_distance, levenshtein_similarity\n",
    "from pytextdist.vector_similarity import cosine_similarity\n",
    "\n",
    "###################################\n",
    "#         CONFIGURATION           #\n",
    "###################################\n",
    "\n",
    "# Configuration file paths (llm_config_anthropic, llm_config_google, llm_config_groq, llm_config_mistral, llm_config_ollama, llm_config_openai)\n",
    "# CONFIG_FILE = \"config/llm_config_openai.json\"\n",
    "MODELS_FILE = \"config/llm_models.json\"\n",
    "CONFIG_RAG_FILE = \"config/llm_config_mistral_rag.json\"\n",
    "CONFIG_RAG_TAVILY_FILE = \"config/secrets-master-llm.json\"\n",
    "VECTOR_DB_TYPE = \"FAISS\" # FAISS, CHROMA\n",
    "CSV_FILE_PATH = \"config/BASE_URL-FULL.csv\"\n",
    "LLM_TYPE = 'Others' # 'Others', 'Ollama' (\n",
    "RAG_CHAT = 'Mistral' # 'OpenAI', 'Mistral', 'LangChain' for Ollama (Same Model as the Trace Generation)\n",
    "TYPE_RETRIEVER = 'faiss' # 'web', 'chroma', 'faiss'\n",
    "\n",
    "# Define an array with all the topics/tools for retrieval\n",
    "vectorstore_topics = [\n",
    "    \"CAEX/AutomationML\",\n",
    "    \"BPMN Designer\",\n",
    "    \"HEPSYCODE\"\n",
    "    # \"Additional Tool 1\",\n",
    "    # \"Additional Tool 2\",\n",
    "    # Add more topics as needed\n",
    "]\n",
    "\n",
    "###################################\n",
    "#         UTILITY FUNCTIONS       #\n",
    "###################################\n",
    "\n",
    "# Function to load configuration from a JSON file\n",
    "def load_config(config_file):\n",
    "    try:\n",
    "        with open(config_file, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Configuration file {config_file} not found.\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Function to load file content\n",
    "def load_file_content(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found.\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to save content to a file\n",
    "def save_to_file(file_path, content):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(content)\n",
    "\n",
    "# Function to save metadata to a file (in JSON format)\n",
    "def save_metadata(file_path, metadata):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(metadata, file, indent=4)\n",
    "\n",
    "###################################\n",
    "#         LLM CONFIGURATION       #\n",
    "###################################\n",
    "\n",
    "# Load LLM configuration\n",
    "config = load_config(CONFIG_RAG_FILE)\n",
    "models_config = load_config(MODELS_FILE)\n",
    "\n",
    "# Extract LLM parameters from configuration\n",
    "LLM = config.get(\"llm\")\n",
    "if not LLM:\n",
    "    raise ValueError(\"LLM name must be specified in the configuration file.\")\n",
    "\n",
    "PRICE_PER_INPUT_TOKEN = config.get(\"price_per_input_token\")\n",
    "PRICE_PER_OUTPUT_TOKEN = config.get(\"price_per_output_token\")\n",
    "temperature = config.get(\"temperature\")\n",
    "max_retries = config.get(\"max_retries\")\n",
    "api_key = config.get(\"api_keys\", {}).get(LLM.lower(), None)\n",
    "base_url = config.get(\"base_url\")\n",
    "\n",
    "# Determine LLM type and initialize LLM instance\n",
    "llm_config = models_config.get(LLM, None)\n",
    "if llm_config and LLM_TYPE != 'Ollama':\n",
    "    llm_params = llm_config.get(\"params\", {})\n",
    "    llm_params[\"temperature\"] = temperature\n",
    "    print('Temperature: ', temperature)\n",
    "    llm_params[\"max_retries\"] = max_retries\n",
    "    llm_params[\"api_key\"] = api_key\n",
    "    llm_params[\"base_url\"] = base_url\n",
    "\n",
    "    # Dynamically initialize the LLM class\n",
    "    llm_class = eval(llm_config[\"class\"])\n",
    "    llm_LangChain = llm_class(**llm_params)\n",
    "    model_name = LLM  # Use LLM name as the model name\n",
    "    print('Model: ', model_name)\n",
    "elif LLM_TYPE == 'Ollama':\n",
    "    llm_params = llm_config.get(\"params\", {})\n",
    "    llm_params[\"temperature\"] = temperature\n",
    "    print('Temperature: ', temperature)\n",
    "    llm_params[\"base_url\"] = base_url\n",
    "\n",
    "    llm_class = eval(llm_config[\"class\"])\n",
    "    llm_LangChain = llm_class(**llm_params)\n",
    "    model_name = LLM\n",
    "    print('Model: ', model_name)\n",
    "else:\n",
    "    raise ValueError(f\"Model configuration for '{LLM}' not found in {MODELS_FILE}.\")\n",
    "\n",
    "###################################\n",
    "#     FEW-SHOT CONFIGURATION      #\n",
    "###################################\n",
    "\n",
    "# Define file paths for static resources and output directories\n",
    "# example_model_path = \"../../01-02-03_MSE/HEPSYCODE/HEPSYCODE-Models/D1/HEPSY/2024-02-14 18.30 13%20-%20FIRFIRGCD_HPV-representations.aird.hepsy\"\n",
    "# example_xes_trace_path = \"../../04_Trace_Parser/D1_HEPSYCODE/XES-MORGAN/2024-02-14 18.30 13%20-%20FIRFIRGCD_HPV-representations.aird.xes\"\n",
    "\n",
    "# Path to the metamodel (Ecore file)\n",
    "metamodel_path = \"../../01_MSE/HEPSYCODE/workspace/org.univaq.hepsy/model/hepsy.ecore\"\n",
    "\n",
    "# Path to the model folders (Hepsy file)\n",
    "# base_model_path = \"../../01-02-03_MSE/HEPSYCODE/HEPSYCODE-Models/D1/HEPSY/\"\n",
    "\n",
    "# Path to the Output Directory\n",
    "base_output_dir = f\"D2-HEPSYCODE-RAG-EVAL-NEW/{TYPE_RETRIEVER}/XES-MORGAN-RAG-LLM-{model_name.lower()}-{temperature}\"\n",
    "base_output_json_dir = f\"D2-HEPSYCODE-RAG-EVAL-NEW/{TYPE_RETRIEVER}/XES-MORGAN-RAG-LLM-{model_name.lower()}-{temperature}/JSON\"\n",
    "\n",
    "# Profiling Folder\n",
    "PROFILING_FOLDER = f\"D2-HEPSYCODE-RAG-EVAL-NEW/{TYPE_RETRIEVER}/XES-MORGAN-RAG-LLM-{model_name.lower()}-{temperature}/JSON\"\n",
    "if not os.path.exists(PROFILING_FOLDER):\n",
    "    os.makedirs(PROFILING_FOLDER)\n",
    "PROFILING_CSV_FILE = os.path.join(PROFILING_FOLDER, \"profiling.csv\")\n",
    "\n",
    "# CodeCarbon Folder\n",
    "CODECARBON_FOLDER  = f\"D2-HEPSYCODE-RAG-EVAL-NEW/{TYPE_RETRIEVER}/XES-MORGAN-RAG-LLM-{model_name.lower()}-{temperature}/JSON\"\n",
    "if not os.path.exists(CODECARBON_FOLDER ):\n",
    "    os.makedirs(CODECARBON_FOLDER )\n",
    "PROFILING_CSV_FILE = os.path.join(PROFILING_FOLDER, \"codecarbon_summary.csv\")\n",
    "\n",
    "# Folder to save evaluation results per file\n",
    "EVALUATION_FOLDER = f\"D2-HEPSYCODE-RAG-EVAL-NEW/{TYPE_RETRIEVER}/XES-MORGAN-RAG-LLM-{model_name.lower()}-{temperature}/JSON\"\n",
    "if not os.path.exists(EVALUATION_FOLDER):\n",
    "    os.makedirs(EVALUATION_FOLDER)\n",
    "\n",
    "# Global variable to force context regeneration regardless of REFINED_CONTEXT_PATH presence\n",
    "FORCE_CONTEXT_GEN = True  # Set to True to force context generation even if REFINED_CONTEXT_PATH exists\n",
    "\n",
    "# File path to save the refined context (persistent cache)\n",
    "REFINED_CONTEXT_PATH = f\"D2-HEPSYCODE-RAG-EVAL-NEW/{TYPE_RETRIEVER}/XES-MORGAN-RAG-LLM-{model_name.lower()}-{temperature}/HEPSYCODE_refined_context.json\"\n",
    "# REFINED_CONTEXT_PATH = \"config/CAEX_refined_context.json\"\n",
    "# REFINED_CONTEXT_PATH = \"config/BPMN_Designer_refined_context.json\"\n",
    "\n",
    "###################################\n",
    "#         GLOBAL PROFILING        #\n",
    "###################################\n",
    "\n",
    "# Global list to collect CodeCarbon metrics for each node call (per file)\n",
    "cc_metrics_for_file = []  # This will be reset for each file\n",
    "\n",
    "# Global list for overall CodeCarbon summary per file\n",
    "cc_summary_records = []\n",
    "\n",
    "# Global list to save profiling data\n",
    "profiling_records = []\n",
    "\n",
    "###################################\n",
    "#      TIMING NODE PROFILING      #\n",
    "###################################\n",
    "\n",
    "def timing_profile_node(func):\n",
    "    \"\"\"\n",
    "    Decorator to profile a node function.\n",
    "    Appends a record with the node name and its execution time (in seconds) to profiling_records.\n",
    "    \"\"\"\n",
    "    def wrapper(state, *args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(state, *args, **kwargs)\n",
    "        end = time.time()\n",
    "        elapsed = end - start\n",
    "        profiling_records.append({\"node\": func.__name__, \"execution_time\": elapsed})\n",
    "        print(f\"[Profiling] {func.__name__} took {elapsed:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "###################################\n",
    "#    CODECARBON NODE DECORATOR    #\n",
    "###################################\n",
    "\n",
    "# os.environ[\"CODECARBON_API_KEY\"] = \"CODECARBON_API_KEY\"\n",
    "# os.environ[\"CODECARBON_API_URL\"] = \"https://api.codecarbon.io\"\n",
    "# os.environ[\"CODECARBON_EXPERIMENT_ID\"] = \"UUID\"\n",
    "\n",
    "def cc_profile_node(func):\n",
    "    \"\"\"\n",
    "    Decorator that wraps a node function with CodeCarbon tracking.\n",
    "    It starts a tracker before calling the node and stops it right after.\n",
    "    The resulting metrics are appended to the global cc_metrics_for_file list.\n",
    "    \"\"\"\n",
    "    def wrapper(state, *args, **kwargs):\n",
    "        # Create a CodeCarbon tracker for this node\n",
    "        tracker = EmissionsTracker(\n",
    "            project_name=f\"cc_{func.__name__}\",\n",
    "            measure_power_secs=1,\n",
    "            output_dir=CODECARBON_FOLDER,  # You can adjust output_dir as needed (\".\")\n",
    "            allow_multiple_runs=True\n",
    "            # api_call_interval=4,\n",
    "            # experiment_id=experiment_id,\n",
    "            # save_to_api=True\n",
    "        )\n",
    "        tracker.start()\n",
    "        result = func(state, *args, **kwargs)\n",
    "        emissions = tracker.stop()\n",
    "        # Try to extract detailed metrics if available (from the internal attribute)\n",
    "        if hasattr(tracker, \"_final_emissions_data\"):\n",
    "            metrics = tracker._final_emissions_data\n",
    "        else:\n",
    "            metrics = {\"total_emissions\": emissions}\n",
    "        # Append the node's CodeCarbon metrics to the global list\n",
    "        cc_metrics_for_file.append({\n",
    "            \"node\": func.__name__,\n",
    "            **metrics  # Flatten the metrics dictionary\n",
    "        })\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "###################################\n",
    "#       PROFILE & CC DECORATORS   #\n",
    "###################################\n",
    "\n",
    "# (Assuming you already have a @profile_node decorator for timing, as in your code.)\n",
    "# Here we combine both decorators so that each node is profiled for time and CodeCarbon metrics.\n",
    "# The order of decorators means that cc_profile_node will wrap the function first.\n",
    "def profile_node(func):\n",
    "    return timing_profile_node(cc_profile_node(func))\n",
    "\n",
    "###################################\n",
    "#       LOAD URLS FROM CSV        #\n",
    "###################################\n",
    "\n",
    "def load_urls_from_csv(csv_file_path):\n",
    "    urls = []\n",
    "    try:\n",
    "        with open(csv_file_path, 'r', newline='', encoding='utf-8') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            for row in reader:\n",
    "                if row:  # Ensure the row is not empty\n",
    "                    urls.append(row[0].strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file '{csv_file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "    return urls\n",
    "\n",
    "# Load base URLs for the vector database from CSV\n",
    "BASE_URLS = load_urls_from_csv(CSV_FILE_PATH)\n",
    "if not BASE_URLS:\n",
    "    raise ValueError(\"No URLs were loaded from the CSV file.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(BASE_URLS)} URLs from '{CSV_FILE_PATH}'.\")\n",
    "\n",
    "###################################\n",
    "# RAG AGENT SETUP (Chroma/FAISS)  #\n",
    "###################################\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "if VECTOR_DB_TYPE == \"CHROMA\":\n",
    "    # Directory for persisting the Chroma vector store.\n",
    "    CHROMA_PERSIST_DIR = \"chroma_db\"\n",
    "    \n",
    "    # Load RAG configuration\n",
    "    config_rag = load_config(CONFIG_RAG_FILE)\n",
    "    api_key_rag = config_rag.get(\"api_keys\", {}).get(LLM.lower(), None)\n",
    "    \n",
    "    # Initialize OpenAIEmbeddings\n",
    "    embd = OpenAIEmbeddings(openai_api_key=api_key_rag)\n",
    "    \n",
    "    # Build or load the Chroma vector store\n",
    "    if os.path.exists(CHROMA_PERSIST_DIR) and os.listdir(CHROMA_PERSIST_DIR):\n",
    "        print(\"Loading existing Chroma vector store from disk...\")\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            embedding_function=embd,\n",
    "            collection_name=\"rag-chroma\"\n",
    "        )\n",
    "        retriever = vectorstore.as_retriever()\n",
    "    else:\n",
    "        print(\"Creating new Chroma vector store...\")\n",
    "        docs = [WebBaseLoader(url).load() for url in BASE_URLS]\n",
    "        docs_list = [item for sublist in docs for item in sublist]\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=500, chunk_overlap=0\n",
    "        )\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=doc_splits,\n",
    "            collection_name=\"rag-chroma\",\n",
    "            embedding=embd,\n",
    "            persist_directory=CHROMA_PERSIST_DIR\n",
    "        )\n",
    "        retriever = vectorstore.as_retriever()\n",
    "elif VECTOR_DB_TYPE == \"FAISS\":\n",
    "\n",
    "    # Load RAG configuration\n",
    "    config_rag = load_config(CONFIG_RAG_FILE)\n",
    "    api_key_rag = config_rag.get(\"api_keys\", {}).get(LLM.lower(), None)\n",
    "    \n",
    "    EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"  \n",
    "    HUGGINGFACE_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    \n",
    "    faiss_folder = \"faiss\"\n",
    "    if not os.path.exists(faiss_folder):\n",
    "        os.makedirs(faiss_folder)\n",
    "        print(f\"Folder '{faiss_folder}' created.\")\n",
    "    else:\n",
    "        print(f\"The folder '{faiss_folder}' already exists.\")\n",
    "    \n",
    "    DATABASE_PATH = os.path.join(faiss_folder, \"faiss_index.index\")\n",
    "    METADATA_PATH = os.path.join(faiss_folder, \"metadata.json\")\n",
    "    \n",
    "    embedding = HuggingFaceEmbeddings(model_name=HUGGINGFACE_MODEL_NAME)\n",
    "    \n",
    "    if os.path.exists(DATABASE_PATH):\n",
    "        print(\"Loading existing FAISS index from disk...\")\n",
    "        vectorstore = FAISS.load_local(DATABASE_PATH, embedding, allow_dangerous_deserialization=True)\n",
    "        if os.path.exists(METADATA_PATH):\n",
    "            with open(METADATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "    else:\n",
    "        print(\"Creating new FAISS vector store...\")\n",
    "        from langchain_community.document_loaders import WebBaseLoader\n",
    "        docs = [WebBaseLoader(url).load() for url in BASE_URLS]\n",
    "        docs_list = [item for sublist in docs for item in sublist]\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=0)\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        vectorstore = FAISS.from_documents(doc_splits, embedding)\n",
    "        vectorstore.save_local(DATABASE_PATH)\n",
    "        metadata = [doc.metadata for doc in doc_splits]\n",
    "        with open(METADATA_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "        \n",
    "    retriever = vectorstore.as_retriever()    \n",
    "\n",
    "###################################\n",
    "#         ROUTER NODE             #\n",
    "###################################\n",
    "\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Data model for routing the user query\n",
    "class RouteQuery(BaseModel):\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Route the user query to either a vectorstore or web search.\"\n",
    "    )\n",
    "\n",
    "# Initialize RAG LLM and router\n",
    "LLM_RAG = config_rag.get(\"llm\")\n",
    "LLM_RAG_TEMP = config_rag.get(\"temperature\")\n",
    "\n",
    "if RAG_CHAT == 'OpenAI':\n",
    "    llm_rag = ChatOpenAI(model=LLM_RAG, temperature=LLM_RAG_TEMP)\n",
    "elif RAG_CHAT == 'LangChain':\n",
    "    llm_rag = OllamaFunctions(model=LLM, temperature=LLM_RAG_TEMP) \n",
    "elif RAG_CHAT == 'Mistral':\n",
    "    llm_rag = llm_LangChain\n",
    "\n",
    "structured_llm_router = llm_rag.with_structured_output(RouteQuery)\n",
    "\n",
    "# Join the topics into a single string, separated by commas\n",
    "topics_str = \", \".join(vectorstore_topics)\n",
    "\n",
    "# Create router prompt\n",
    "router_system_prompt = (\n",
    "    \"You are an expert at routing user queries to either a vectorstore or web search. \"\n",
    "    \"The vectorstore contains documents related to {topics_str}.\"\n",
    "    \"Use the vectorstore for questions on these topics; otherwise, use web search.\"\n",
    "    \"Based on the query, respond with a JSON object that contains a key 'datasource'\"\n",
    "    \"whose value is either 'vectorstore' or 'web_search'. Do not include any additional keys or text.\"\n",
    ")\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", router_system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "question_router = route_prompt | structured_llm_router\n",
    "\n",
    "###################################\n",
    "#      RETRIEVAL GRADER NODE      #\n",
    "###################################\n",
    "\n",
    "# Data model for grading document relevance\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Indicates whether the document is relevant ('yes' or 'no').\"\n",
    "    )\n",
    "\n",
    "structured_llm_grader = llm_rag.with_structured_output(GradeDocuments)\n",
    "\n",
    "grader_system_prompt = (\n",
    "    \"You are a grader assessing the relevance of a retrieved document to a user query. \"\n",
    "    \"If the document contains keywords or semantic content related to the user query, grade it as relevant. \"\n",
    "    \"Output a binary score 'yes' or 'no'.\"\n",
    ")\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grader_system_prompt),\n",
    "        (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser query:\\n{question}\"),\n",
    "    ]\n",
    ")\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "###################################\n",
    "#        GENERATION CHAIN         #\n",
    "###################################\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "# Set up in-memory cache to avoid repeating expensive LLM calls.\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "\"\"\"\n",
    "context_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You are an expert in information retrieval and content synthesis. Your task is to refine and enhance context \"\n",
    "                \"from multiple sources by generating a cohesive, well-structured, and detailed context that combines information \"\n",
    "                \"from various retrieved documents.\\n\\n\"\n",
    "                \"Responsibilities:\\n\"\n",
    "                \"1. Synthesize information from multiple sources into a unified explanation.\\n\"\n",
    "                \"2. Expand on the query with relevant details from the retrieved content.\\n\"\n",
    "                \"3. Format the refined context with clear structure and professional language.\\n\"\n",
    "                \"4. Incorporate metadata for traceability.\\n\"\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            (\n",
    "                \"Question: {question}\\n\\n\"\n",
    "                \"The following are the retrieved documents and metadata:\\n\\n{context}\\n\\n\"\n",
    "                \"Using this information, generate a refined and comprehensive context.\"\n",
    "            )\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "context_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You are an expert in information retrieval and content synthesis. Your task is to generate a comprehensive, detailed, \"\n",
    "                \"and well-structured context by combining information from various sources, starting from the query generated from the metamodel. \"\n",
    "                \"The final context should include at least the following elements:\\n\\n\"\n",
    "                \"1. **Overview**: a general overview of the subject.\\n\"\n",
    "                \"2. **Tool Name**: derived directly from the package name in the metamodel.\\n\"\n",
    "                \"3. **Technical Details**: information on technologies, tools, methodologies, etc.\\n\"\n",
    "                \"4. **Installation and Setup Instructions**: how to download, configure, and install the tool.\\n\"\n",
    "                \"5. **System Requirements**: dependencies and supported platforms.\\n\"\n",
    "                \"6. **Tutorial**: step-by-step instructions or a guide on how to effectively use the tool.\\n\"\n",
    "                \"7. **References**: links and documentation sources.\\n\\n\"\n",
    "                \"Use professional and technical language, and format the output in markdown.\"\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            (\n",
    "                \"Query generated from the metamodel:\\n{question}\\n\\n\"\n",
    "                \"Retrieved documents and metadata:\\n{context}\\n\\n\"\n",
    "                \"Using this information, generate a refined and complete context that follows the structure indicated above.\"\n",
    "            )\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Load static content files\n",
    "metamodel_text = load_file_content(metamodel_path)\n",
    "\n",
    "context_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You are a domain model expert with deep knowledge of metamodeling and semantic model interpretation.\"\n",
    "                \"Your task is to analyze a given Ecore metamodel and extract a structured and comprehensive domain context.\"\n",
    "                \"This context will be used as background knowledge to guide synthetic model generation.\\n\\n\"\n",
    "                \"Instructions:\\n\"\n",
    "                \"1. Identify and describe the key domain concepts (classes, attributes, references, enumerations).\\n\"\n",
    "                \"2. Explain the relationships and constraints implied by the metamodel structure.\\n\"\n",
    "                \"3. Enrich the context with relevant external domain knowledge.\\n\"\n",
    "                \"4. Structure the output clearly with sections like 'Domain Overview', 'Key Concepts', 'Relationships', 'Behavioral Semantics', and 'External Domain Background'.\\n\"\n",
    "                \"5. When possible, infer the real-world domain (e.g., hardware modeling, system behavior, message passing) and include related terminology.\\n\"\n",
    "                \"6. Use professional and academic language, suitable for expert systems.\\n\"\n",
    "                \"7. Incorporate any relevant metadata such as names, types, multiplicities, and containment relationships.\"\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            (\n",
    "                \"Here is the Ecore metamodel to analyze:\\n\\n\"\n",
    "                \"{metamodel_text}\\n\\n\"\n",
    "                \"Question: {question}\\n\\n\"\n",
    "                \"The following are the retrieved documents and metadata:\\n\\n{context}\\n\\n\"\n",
    "                \"Using this, generate a refined and detailed context about the domain it represents.\"\n",
    "            )\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "if RAG_CHAT == 'OpenAI':\n",
    "    llm_for_context = ChatOpenAI(model=LLM_RAG, temperature=LLM_RAG_TEMP)\n",
    "elif RAG_CHAT == 'LangChain':\n",
    "    llm_for_context = llm_LangChain\n",
    "elif RAG_CHAT == 'Mistral':\n",
    "    llm_for_context = llm_LangChain\n",
    "    \n",
    "rag_chain = context_prompt_template | llm_for_context | StrOutputParser()\n",
    "\n",
    "###################################\n",
    "#     HALLUCINATION GRADER        #\n",
    "###################################\n",
    "\n",
    "# Data model for grading hallucination\n",
    "class GradeHallucinations(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Indicates if the answer is grounded in facts ('yes' or 'no').\"\n",
    "    )\n",
    "\n",
    "structured_llm_hallucination = llm_rag.with_structured_output(GradeHallucinations)\n",
    "\n",
    "hallucination_system_prompt = (\n",
    "    \"You are a grader assessing whether the LLM generation is grounded in the retrieved facts. \"\n",
    "    \"Output a binary score 'yes' if the answer is supported by the facts, otherwise 'no'.\"\n",
    ")\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", hallucination_system_prompt),\n",
    "        (\"human\", \"Facts:\\n\\n{documents}\\n\\nLLM Generation:\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "hallucination_grader = hallucination_prompt | structured_llm_hallucination\n",
    "\n",
    "###################################\n",
    "#         ANSWER GRADER           #\n",
    "###################################\n",
    "\n",
    "# Data model for grading answer relevance\n",
    "class GradeAnswer(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Indicates if the answer addresses the question ('yes' or 'no').\"\n",
    "    )\n",
    "\n",
    "structured_llm_answer = llm_rag.with_structured_output(GradeAnswer)\n",
    "\n",
    "answer_system_prompt = (\n",
    "    \"You are a grader assessing whether an LLM-generated answer addresses the user query. \"\n",
    "    \"Output a binary score 'yes' if it does, otherwise 'no'.\"\n",
    ")\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", answer_system_prompt),\n",
    "        (\"human\", \"User Query:\\n{question}\\n\\nLLM Generation:\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "answer_grader = answer_prompt | structured_llm_answer\n",
    "\n",
    "###################################\n",
    "#       QUESTION REWRITER         #\n",
    "###################################\n",
    "\n",
    "rewrite_system_prompt = (\n",
    "    \"You are a question rewriter. Given an input question, produce an improved version optimized for vectorstore retrieval. \"\n",
    "    \"Focus on the underlying semantic intent.\"\n",
    ")\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rewrite_system_prompt),\n",
    "        (\"human\", \"Original question:\\n{question}\\n\\nRewrite the question:\"),\n",
    "    ]\n",
    ")\n",
    "question_rewriter = rewrite_prompt | llm_for_context | StrOutputParser()\n",
    "\n",
    "###################################\n",
    "#           WEB SEARCH            #\n",
    "###################################\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "config_tavily = load_config(CONFIG_RAG_TAVILY_FILE)\n",
    "os.environ[\"TAVILY_API_KEY\"] = config_tavily.get(\"tavily_api_key\")\n",
    "web_search_tool = TavilySearchResults(k=3)\n",
    "\n",
    "###################################\n",
    "#       GRAPH STATE DEFINITION    #\n",
    "###################################\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict, total=False): \n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[Any]\n",
    "    file_name: str      # For trace generation node\n",
    "    context_llm: str    # The refined context\n",
    "    trace_status: str\n",
    "    xes_trace: str\n",
    "    metadata: Dict[str, Any]\n",
    "    branch: str         # Indicates the branch: \"retrieve\" or \"web_search\"\n",
    "    evaluation_metrics: Dict[str, float]\n",
    "    bert_score: Dict[str, float]\n",
    "    web_bert_score: Dict[str, float]\n",
    "    skip_router: bool   # If True, skip the routing and proceed directly to cache_context_node\n",
    "\n",
    "###################################\n",
    "# FUNCTION: GENERATE QUERY FROM METAMODEL #\n",
    "###################################\n",
    "\n",
    "@profile_node\n",
    "def generate_query_from_metamodel(metamodel_path):\n",
    "    \"\"\"\n",
    "    Reads the metamodel (Ecore file), extracts basic information (package name, nsURI, and classifiers),\n",
    "    and constructs a query based solely on the extracted information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(metamodel_path)\n",
    "        root = tree.getroot()\n",
    "        # The root is typically an EPackage with attributes\n",
    "        package_name = root.attrib.get(\"name\", \"UnknownPackage\")\n",
    "        ns_uri = root.attrib.get(\"nsURI\", \"Unknown nsURI\")\n",
    "        # Extract all classifiers (e.g., EClass, EEnum, etc.)\n",
    "        classifiers = []\n",
    "        for classifier in root.findall(\"{http://www.eclipse.org/emf/2002/Ecore}eClassifiers\"):\n",
    "            classifiers.append(classifier.attrib.get(\"name\", \"UnnamedClassifier\"))\n",
    "        classifiers_str = \", \".join(classifiers) if classifiers else \"None\"\n",
    "        # Build a generic query based solely on the metamodel information\n",
    "        query = (\n",
    "            f\"Metamodel Analysis Query:\\n\"\n",
    "            f\"Package Name: {package_name}\\n\"\n",
    "            f\"Namespace URI: {ns_uri}\\n\"\n",
    "            f\"Classifiers: {classifiers_str}\\n\"\n",
    "            \"Based solely on the metamodel information provided above, generate a context for a tool based on this metamodel. \"\n",
    "            \"The context should include the tool's name, which must be directly derived from the package name.\"\n",
    "        )\n",
    "        # query = (\"HEPSYCODE\")\n",
    "        return query\n",
    "    except Exception as e:\n",
    "        print(\"Error generating query from metamodel:\", e)\n",
    "        return \"Metamodel analysis query could not be generated.\"\n",
    "\n",
    "# The query for generating the context is now created based on the metamodel\n",
    "# question = generate_query_from_metamodel(metamodel_path)\n",
    "# print(question)\n",
    "\n",
    "###################################\n",
    "#        EVALUATION NODES         #\n",
    "###################################\n",
    "\n",
    "# (1) LLM-based Evaluation for RAG output (vectorstore branch)\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class RAGEvaluationMetrics(BaseModel):\n",
    "    faithfulness: float = Field(..., description=\"Score (0-1) indicating how faithful the answer is to the facts.\")\n",
    "    answer_relevance: float = Field(..., description=\"Score (0-1) indicating how well the answer addresses the question.\")\n",
    "    context_precision: float = Field(..., description=\"Score (0-1) representing the precision of the context used.\")\n",
    "    context_accuracy: float = Field(..., description=\"Score (0-1) representing the accuracy of the retrieved context.\")\n",
    "    context_recall: float = Field(..., description=\"Score (0-1) representing the recall of the context.\")\n",
    "    context_f1: float = Field(..., description=\"Score (0-1) representing the F1 measure of the context.\")\n",
    "\n",
    "@profile_node\n",
    "def evaluate_rag_output(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Node to evaluate the RAG output (vectorstore branch) based on metrics such as:\n",
    "    Faithfulness, Answer Relevance, Context Precision, Context Accuracy,\n",
    "    Context Recall, and Context F1.\n",
    "    \"\"\"\n",
    "    print(\"--- EVALUATE RAG OUTPUT METRICS ---\")\n",
    "    question_val = state.get(\"question\", \"\")\n",
    "    generation = state.get(\"generation\", \"\")\n",
    "    documents = state.get(\"documents\", [])\n",
    "    context_text = \"\\n\".join([doc.page_content for doc in documents]) if documents else \"\"\n",
    "    \n",
    "    eval_input = {\n",
    "         \"question\": question_val,\n",
    "         \"generation\": generation,\n",
    "         \"context\": context_text\n",
    "    }\n",
    "    \n",
    "    eval_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", (\n",
    "            \"You are an expert evaluator of RAG outputs. Evaluate the output based on the following metrics: \"\n",
    "            \"Faithfulness, Answer Relevance, Context Precision, Context Accuracy, Context Recall, and Context F1. \"\n",
    "            \"For each metric, assign a score between 0 and 1. \"\n",
    "            \"Respond in JSON format with keys: faithfulness, answer_relevance, context_precision, \"\n",
    "            \"context_accuracy, context_recall, context_f1.\"\n",
    "        )),\n",
    "        (\"user\", \"Question:\\n{question}\\n\\nGenerated Answer:\\n{generation}\\n\\nContext:\\n{context}\\n\\nProvide the evaluation:\")\n",
    "    ])\n",
    "\n",
    "    structured_eval = llm_rag.with_structured_output(RAGEvaluationMetrics)\n",
    "    eval_chain = eval_prompt | structured_eval\n",
    "    try:    \n",
    "        eval_metrics = eval_chain.invoke(eval_input)\n",
    "        state[\"evaluation_metrics\"] = eval_metrics.dict()\n",
    "        print(\"Evaluation metrics:\", state[\"evaluation_metrics\"])\n",
    "    except Exception as e:\n",
    "         print(\"Error during evaluation of RAG output:\", e)\n",
    "         state[\"evaluation_metrics\"] = {}\n",
    "    return state\n",
    "\n",
    "# (2) BERTScore Evaluation for RAG output (vectorstore branch)\n",
    "@profile_node\n",
    "def evaluate_bert_score(state: GraphState) -> GraphState:\n",
    "    print(\"--- EVALUATE BERT SCORE ---\")\n",
    "    try:\n",
    "        from bert_score import score\n",
    "    except ImportError:\n",
    "        print(\"Please install bert-score using 'pip install bert-score'\")\n",
    "        state[\"bert_score\"] = None\n",
    "        return state\n",
    "\n",
    "    candidate = state.get(\"generation\", \"\")\n",
    "    \n",
    "    # documents = state.get(\"documents\", [])\n",
    "    # reference = \"\\n\".join([doc.page_content for doc in documents]) if documents else \"\"\n",
    "\n",
    "    # Load the reference context from an external JSON file\n",
    "    REFERENCE_FOLDER = \"config\"  # Folder containing the reference file\n",
    "    REFERENCE_FILE = \"HEPSYCODE_refined_context_Checked.json\"        # Name of the JSON file\n",
    "    REFERENCE_CONTEXT_PATH = os.path.join(REFERENCE_FOLDER, REFERENCE_FILE)\n",
    "    \n",
    "    try:\n",
    "        with open(REFERENCE_CONTEXT_PATH, \"r\", encoding=\"utf-8\") as ref_file:\n",
    "            ref_data = json.load(ref_file)\n",
    "        # Expecting the JSON file to have a key \"reference_context\" with the reference text\n",
    "        reference = ref_data.get(\"context\", \"\")\n",
    "        print(\"RAG context: \",reference)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading reference context from {REFERENCE_CONTEXT_PATH}: {e}\")\n",
    "        state[\"bert_score\"] = None\n",
    "        return state\n",
    "\n",
    "    if not candidate or not reference:\n",
    "        print(\"Candidate or reference text is empty. Skipping BERTScore evaluation.\")\n",
    "        state[\"bert_score\"] = None\n",
    "        return state\n",
    "    \n",
    "    P, R, F1 = score([candidate], [reference], lang=\"en\", verbose=True)\n",
    "    bert_precision = P[0].item()\n",
    "    bert_recall = R[0].item()\n",
    "    bert_f1 = F1[0].item()\n",
    "\n",
    "    # Calculate cosine similarity using TextDistance on a bag-of-words representation\n",
    "    # The texts are split by whitespace to form lists of words\n",
    "    cosine_similarity = textdistance.cosine.normalized_similarity(candidate.split(), reference.split())\n",
    "    # cosine_similarity = cosine_similarity((str(candidate.split()), str(reference.split()))\n",
    "    \n",
    "    # Calculate the Levenshtein distance using TextDistance\n",
    "    levenshtein_distance = textdistance.levenshtein.normalized_similarity(candidate, reference)\n",
    "    # levenshtein_distance = levenshtein_similarity(str(candidate), str(reference))\n",
    "\n",
    "    # Add all computed metrics into the state\n",
    "    #state[\"web_bert_score\"] = {\"precision\": web_bert_precision, \"recall\": web_bert_recall, \"f1\": web_bert_f1}\n",
    "    state[\"bert_score\"] = {\n",
    "        \"precision\": bert_precision,\n",
    "        \"recall\": bert_recall,\n",
    "        \"f1\": bert_f1,\n",
    "        \"cosine_similarity\": cosine_similarity,\n",
    "        \"levenshtein_distance\": levenshtein_distance\n",
    "    }\n",
    "    print(\"RAG BERTScore metrics:\", state[\"bert_score\"])\n",
    "    \n",
    "    # state[\"bert_score\"] = {\"precision\": bert_precision, \"recall\": bert_recall, \"f1\": bert_f1}\n",
    "    # print(\"BERTScore metrics:\", state[\"bert_score\"])\n",
    "    return state\n",
    "\n",
    "# (3) LLM-based Evaluation for Web Search output\n",
    "# Here, we introduce an additional metric \"accuracy\" along with the previous ones.\n",
    "class WebEvaluationMetrics(BaseModel):\n",
    "    faithfulness: float = Field(..., description=\"Score (0-1) indicating how faithful the answer is to the web sources.\")\n",
    "    answer_relevance: float = Field(..., description=\"Score (0-1) indicating how well the answer addresses the query.\")\n",
    "    context_precision: float = Field(..., description=\"Score (0-1) representing the precision of the web search results.\")\n",
    "    context_accuracy: float = Field(..., description=\"Score (0-1) representing the accuracy of the retrieved web content.\")\n",
    "    context_recall: float = Field(..., description=\"Score (0-1) representing the recall of relevant web information.\")\n",
    "    context_f1: float = Field(..., description=\"Score (0-1) representing the F1 measure of the web search results.\")\n",
    "    accuracy: float = Field(..., description=\"Score (0-1) indicating the overall accuracy of the generated context based on web sources.\")\n",
    "\n",
    "@profile_node\n",
    "def evaluate_web_search_output(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Node to evaluate the output of the web search branch.\n",
    "    It uses the same metrics as the RAG evaluation plus an extra metric 'accuracy'.\n",
    "    The reference is the concatenated web search source content.\n",
    "    \"\"\"\n",
    "    print(\"--- EVALUATE WEB SEARCH OUTPUT METRICS ---\")\n",
    "    question_val = state.get(\"question\", \"\")\n",
    "    generation = state.get(\"generation\", \"\")\n",
    "    documents = state.get(\"documents\", [])\n",
    "    context_text = \"\\n\".join([doc.page_content for doc in documents]) if documents else \"\"\n",
    "    \n",
    "    eval_input = {\n",
    "         \"question\": question_val,\n",
    "         \"generation\": generation,\n",
    "         \"context\": context_text\n",
    "    }\n",
    "    \n",
    "    eval_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", (\n",
    "            \"You are an expert evaluator of web search outputs. Evaluate the output based on the following metrics: \"\n",
    "            \"Faithfulness, Answer Relevance, Context Precision, Context Accuracy, Context Recall, Context F1, and Accuracy. \"\n",
    "            \"For each metric, assign a score between 0 and 1. \"\n",
    "            \"Respond in JSON format with keys: faithfulness, answer_relevance, context_precision, \"\n",
    "            \"context_accuracy, context_recall, context_f1, accuracy.\"\n",
    "        )),\n",
    "        (\"user\", \"Question:\\n{question}\\n\\nGenerated Answer/Context:\\n{generation}\\n\\nWeb Search Sources:\\n{context}\\n\\nProvide the evaluation:\")\n",
    "    ])\n",
    "    \n",
    "    structured_eval = llm_rag.with_structured_output(WebEvaluationMetrics)\n",
    "    eval_chain = eval_prompt | structured_eval\n",
    "    try:  \n",
    "        eval_metrics = eval_chain.invoke(eval_input)\n",
    "        state[\"evaluation_metrics\"] = eval_metrics.dict()\n",
    "        print(\"Web search evaluation metrics:\", state[\"evaluation_metrics\"])\n",
    "    except Exception as e:\n",
    "         print(\"Error during web search evaluation:\", e)\n",
    "         state[\"evaluation_metrics\"] = {}\n",
    "    return state\n",
    "\n",
    "# (4) BERTScore Evaluation for Web Search output\n",
    "@profile_node\n",
    "def evaluate_web_bert_score(state: GraphState) -> GraphState:\n",
    "    print(\"--- EVALUATE WEB BERT SCORE ---\")\n",
    "    try:\n",
    "        from bert_score import score\n",
    "    except ImportError:\n",
    "        print(\"Please install bert-score using 'pip install bert-score'\")\n",
    "        state[\"web_bert_score\"] = None\n",
    "        return state\n",
    "\n",
    "    candidate = state.get(\"generation\", \"\")\n",
    "\n",
    "    \"\"\"\n",
    "    documents = state.get(\"documents\", [])\n",
    "    reference = \"\\n\".join([doc.page_content for doc in documents]) if documents else \"\"\n",
    "    \n",
    "    if not candidate or not reference:\n",
    "        print(\"Candidate or reference text is empty for web search. Skipping BERTScore evaluation.\")\n",
    "        state[\"web_bert_score\"] = None\n",
    "        return state\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the reference context from an external JSON file\n",
    "    REFERENCE_FOLDER = \"config\"  # Folder containing the reference file\n",
    "    REFERENCE_FILE = \"HEPSYCODE_refined_context_Checked.json\"        # Name of the JSON file\n",
    "    REFERENCE_CONTEXT_PATH = os.path.join(REFERENCE_FOLDER, REFERENCE_FILE)\n",
    "    \n",
    "    try:\n",
    "        with open(REFERENCE_CONTEXT_PATH, \"r\", encoding=\"utf-8\") as ref_file:\n",
    "            ref_data = json.load(ref_file)\n",
    "        # Expecting the JSON file to have a key \"reference_context\" with the reference text\n",
    "        reference = ref_data.get(\"context\", \"\")\n",
    "        print(\"Web Context: \",reference)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading reference context from {REFERENCE_CONTEXT_PATH}: {e}\")\n",
    "        state[\"web_bert_score\"] = None\n",
    "        return state\n",
    "\n",
    "    if not candidate or not reference:\n",
    "        print(\"Candidate or reference text is empty. Skipping BERTScore evaluation.\")\n",
    "        state[\"web_bert_score\"] = None\n",
    "        return state\n",
    "    \n",
    "    P, R, F1 = score([candidate], [reference], lang=\"en\", verbose=True)\n",
    "    web_bert_precision = P[0].item()\n",
    "    web_bert_recall = R[0].item()\n",
    "    web_bert_f1 = F1[0].item()\n",
    "\n",
    "    # Calculate cosine similarity using TextDistance on a bag-of-words representation\n",
    "    # The texts are split by whitespace to form lists of words\n",
    "    cosine_similarity = textdistance.cosine.normalized_similarity(candidate.split(), reference.split())\n",
    "    \n",
    "    # Calculate the Levenshtein distance using TextDistance\n",
    "    levenshtein_distance = textdistance.levenshtein(candidate, reference)\n",
    "\n",
    "    # Add all computed metrics into the state\n",
    "    #state[\"web_bert_score\"] = {\"precision\": web_bert_precision, \"recall\": web_bert_recall, \"f1\": web_bert_f1}\n",
    "    state[\"web_bert_score\"] = {\n",
    "        \"precision\": web_bert_precision,\n",
    "        \"recall\": web_bert_recall,\n",
    "        \"f1\": web_bert_f1,\n",
    "        \"cosine_similarity\": cosine_similarity,\n",
    "        \"levenshtein_distance\": levenshtein_distance\n",
    "    }\n",
    "    print(\"Web BERTScore metrics:\", state[\"web_bert_score\"])\n",
    "    return state\n",
    "\n",
    "###################################\n",
    "#        DECIDE TO GENERATE       #\n",
    "###################################\n",
    "\n",
    "def decide_to_generate(state: GraphState) -> str:\n",
    "    print(\"--- DECIDE TO GENERATE ---\")\n",
    "    filtered_documents = state.get(\"documents\", [])\n",
    "    if not filtered_documents:\n",
    "        if branch == \"retrieve\":\n",
    "            print(\"--- No relevant documents found in vectorstore; transforming query to improve retrieval ---\")\n",
    "            return \"transform_query\"\n",
    "        else:  # branch == \"web_search\"\n",
    "            print(\"--- No documents found via web search; proceeding with generation using empty context ---\")\n",
    "            return \"generate\"\n",
    "    else:\n",
    "        print(\"--- Relevant documents found, generating answer ---\")\n",
    "        return \"generate\"\n",
    "\n",
    "###################################\n",
    "#        CACHE NODE (LangGraph)   #\n",
    "###################################\n",
    "\n",
    "@profile_node\n",
    "def cache_context_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    LangGraph node that checks if a refined context is already available.\n",
    "    If present in the state or in the file cache, it uses that value.\n",
    "    Otherwise, it generates the refined context using the rag_chain,\n",
    "    caches it (in state and on disk), and returns the state.\n",
    "    \"\"\"\n",
    "    if \"context_llm\" in state and state[\"context_llm\"]:\n",
    "        print(\"Using refined context already present in state.\")\n",
    "        return state\n",
    "\n",
    "    if os.path.isfile(REFINED_CONTEXT_PATH) and not FORCE_CONTEXT_GEN:\n",
    "        try:\n",
    "            with open(REFINED_CONTEXT_PATH, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            state[\"context_llm\"] = data.get(\"context\", \"\")\n",
    "            print(\"Loaded refined context from file cache (LangGraph node).\")\n",
    "            return state\n",
    "        except Exception as e:\n",
    "            print(\"Error loading refined context from file in cache node:\", e)\n",
    "\n",
    "    print(\"Generating refined context in LangGraph cache node...\")\n",
    "    refined_context = rag_chain.invoke({\"question\": state[\"question\"], \"context\": \"\", \"metamodel_text\": metamodel_text})\n",
    "    state[\"context_llm\"] = refined_context\n",
    "    try:\n",
    "        with open(REFINED_CONTEXT_PATH, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\"context\": refined_context}, f, indent=4, ensure_ascii=False)\n",
    "        print(\"Refined context cached to file from LangGraph node.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error caching refined context to file in cache node:\", e)\n",
    "    return state\n",
    "\n",
    "###################################\n",
    "#          GRAPH NODES            #\n",
    "###################################\n",
    "\n",
    "@profile_node\n",
    "def generate_query_node(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    LangGraph node that generates the query from the metamodel.\n",
    "    If the refined context file exists, skip query generation and mark state to bypass router.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(REFINED_CONTEXT_PATH) and not FORCE_CONTEXT_GEN:\n",
    "        print(\"Refined context file exists. Skipping query generation; proceeding directly to cache_context_node.\")\n",
    "        state[\"skip_router\"] = True  # Flag to skip routing\n",
    "    else:\n",
    "        state[\"skip_router\"] = False  # Flag to skip routing\n",
    "        state[\"question\"] = generate_query_from_metamodel(metamodel_path)\n",
    "        print(\"Generated query from metamodel:\", state[\"question\"])\n",
    "    return state\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "# Node: Retrieve documents using the vectorstore\n",
    "@profile_node\n",
    "def retrieve(state: GraphState) -> GraphState:\n",
    "    print(\"--- RETRIEVE ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    documents = retriever.invoke(question_val)\n",
    "    state[\"documents\"] = documents\n",
    "    return state\n",
    "\n",
    "# Node: Perform web search (remains separate)\n",
    "@profile_node\n",
    "def web_search(state: GraphState) -> GraphState:\n",
    "    print(\"--- WEB SEARCH ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    docs = web_search_tool.invoke({\"query\": question_val})\n",
    "\n",
    "    # Combine web search results into a single Document\n",
    "    \"\"\"\n",
    "    web_results_content = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    from langchain.schema import Document\n",
    "    web_results_doc = Document(page_content=web_results_content)\n",
    "    state[\"documents\"] = [web_results_doc]\n",
    "    return state\n",
    "    \"\"\"\n",
    "    # Check the type of docs and extract content accordingly.\n",
    "    if isinstance(docs, str):\n",
    "        # If docs is a string, use it directly.\n",
    "        web_results_content = docs\n",
    "    elif isinstance(docs, list):\n",
    "        # If docs is a list, check the type of its elements.\n",
    "        if docs and isinstance(docs[0], dict) and \"content\" in docs[0]:\n",
    "            web_results_content = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "        else:\n",
    "            # Assume it's a list of strings.\n",
    "            web_results_content = \"\\n\".join(docs)\n",
    "    else:\n",
    "        # Fallback: convert docs to string.\n",
    "        web_results_content = str(docs)\n",
    "    \n",
    "    from langchain.schema import Document\n",
    "    web_results_doc = Document(page_content=web_results_content)\n",
    "    state[\"documents\"] = [web_results_doc]\n",
    "    return state    \n",
    "\n",
    "# Merged Node: Generate answer using the RAG chain (used for both branches)\n",
    "def generate(state: GraphState) -> GraphState:\n",
    "    print(\"--- GENERATE (RAG) ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question_val, \"metamodel_text\": metamodel_text})\n",
    "    state[\"generation\"] = generation\n",
    "    return state\n",
    "\n",
    "# Node: Generate answer using web search results\n",
    "@profile_node\n",
    "def generate_web(state: GraphState) -> GraphState:\n",
    "    print(\"--- GENERATE (Web) ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question, \"metamodel_text\": metamodel_text})\n",
    "    state[\"generation\"] = generation\n",
    "    return state\n",
    "\n",
    "# Node: Grade documents for relevance\n",
    "\"\"\"\n",
    "@profile_node\n",
    "def grade_documents(state: GraphState) -> GraphState:\n",
    "    print(\"--- GRADE DOCUMENTS ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question_val, \"document\": d.page_content})\n",
    "        if score is None:\n",
    "            print(\"The retrieval_grader did not return a valid result for this document, skipping it.\")\n",
    "        continue\n",
    "        if score.binary_score.lower() == \"yes\":\n",
    "            print(\"--- Document is relevant ---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"--- Document is not relevant ---\")\n",
    "    state[\"documents\"] = filtered_docs\n",
    "    return state\n",
    "\"\"\"\n",
    "\n",
    "@profile_node\n",
    "def grade_documents(state: GraphState) -> GraphState:\n",
    "    print(\"--- GRADE DOCUMENTS ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        attempts = 0\n",
    "        max_attempts = 3\n",
    "        score = None\n",
    "        while attempts < max_attempts:\n",
    "            try:\n",
    "                score = retrieval_grader.invoke({\"question\": question_val, \"document\": d.page_content})\n",
    "                break  # Exit loop if the call is successful\n",
    "            except httpx.HTTPStatusError as e:\n",
    "                if e.response.status_code == 429:\n",
    "                    wait_time = 10  # seconds to wait; adjust if needed\n",
    "                    print(f\"Rate limit exceeded, waiting for {wait_time} seconds before retrying...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    attempts += 1\n",
    "                else:\n",
    "                    raise  # Re-raise the error if it's not a 429\n",
    "            except httpx.ReadTimeout as e:\n",
    "                wait_time = 10  # seconds to wait; adjust if needed\n",
    "                print(f\"Read timeout occurred, waiting for {wait_time} seconds before retrying...\")\n",
    "                time.sleep(wait_time)\n",
    "                attempts += 1\n",
    "        if score is None:\n",
    "            print(\"The retrieval_grader did not return a valid result for this document, skipping it.\")\n",
    "            continue\n",
    "        if score.binary_score.lower() == \"yes\":\n",
    "            print(\"--- Document is relevant ---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"--- Document is not relevant ---\")\n",
    "    state[\"documents\"] = filtered_docs\n",
    "    return state\n",
    "\n",
    "# Merged Node: Transform the query (for both branches)\n",
    "@profile_node\n",
    "def transform_query(state: GraphState) -> GraphState:\n",
    "    print(\"--- TRANSFORM QUERY (RAG) ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    better_question = question_rewriter.invoke({\"question\": question_val})\n",
    "    print(better_question)\n",
    "    state[\"question\"] = better_question\n",
    "    return state\n",
    "\n",
    "# Node: Transform the query for web search\n",
    "@profile_node\n",
    "def transform_query_web(state: GraphState) -> GraphState:\n",
    "    print(\"--- TRANSFORM QUERY (Web) ---\")\n",
    "    question = state[\"question\"]\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    print(better_question)\n",
    "    state[\"question\"] = better_question\n",
    "    return state\n",
    "\n",
    "# Conditional routing after transformation: based on branch in state\n",
    "\"\"\"\n",
    "def route_after_transform(state: GraphState) -> str:\n",
    "    if state.get(\"branch\") == \"retrieve\":\n",
    "        return \"retrieve\"\n",
    "    elif state.get(\"branch\") == \"web_search\":\n",
    "        return \"web_search\"\n",
    "    return \"retrieve\"\n",
    "\"\"\"\n",
    "\n",
    "# Node: Grade the generation against the documents and question\n",
    "@profile_node\n",
    "def grade_generation_v_documents_and_question(state: GraphState) -> str:\n",
    "    print(\"--- GRADE GENERATION ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    # Evaluate if the generation is supported by the retrieved documents.\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    if score.binary_score.lower() == \"yes\":\n",
    "        print(\"--- Generation is grounded in documents ---\")\n",
    "        # score_answer = answer_grader.invoke({\"question\": question_val, \"generation\": generation})\n",
    "        # if score_answer.binary_score.lower() == \"yes\":\n",
    "        #     print(\"--- Generation addresses the question ---\")\n",
    "        return \"useful\"\n",
    "        # else:\n",
    "        #     print(\"--- Generation does not address the question ---\")\n",
    "        #     return \"not useful\"\n",
    "    else:\n",
    "        print(\"--- Generation is not supported by documents, retrying ---\")\n",
    "        return \"not useful\"\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "###################################\n",
    "#       GRAPH WORKFLOW SETUP      #\n",
    "###################################\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# Initialize the state graph using our GraphState type\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "\n",
    "# Add the new node to the workflow\n",
    "workflow.add_node(\"generate_query\", generate_query_node)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)             # Merged generate node\n",
    "workflow.add_node(\"generate_web\", generate_web)\n",
    "workflow.add_node(\"transform_query\", transform_query) # Merged transform node\n",
    "workflow.add_node(\"transform_query_web\", transform_query_web)\n",
    "workflow.add_node(\"cache_context\", cache_context_node)  # Caching node\n",
    "# workflow.add_node(\"trace_generation\", trace_generation_node)\n",
    "\n",
    "# Add evaluation nodes for vectorstore branch\n",
    "workflow.add_node(\"evaluate_rag_output\", evaluate_rag_output)\n",
    "workflow.add_node(\"evaluate_bert_score\", evaluate_bert_score)\n",
    "\n",
    "# Add evaluation nodes for web search branch\n",
    "workflow.add_node(\"evaluate_web_search_output\", evaluate_web_search_output)\n",
    "workflow.add_node(\"evaluate_web_bert_score\", evaluate_web_bert_score)\n",
    "\n",
    "def custom_parse_router_output(raw_output: str) -> RouteQuery:\n",
    "    try:\n",
    "        data = json.loads(raw_output)\n",
    "        # If the output contains \"datasource\", use it directly.\n",
    "        if \"datasource\" in data:\n",
    "            return RouteQuery(**data)\n",
    "        # Alternatively, if the output contains a \"tool\" key and it equals \"HEPSYCODE\",\n",
    "        # you might decide to map it to one of your expected values.\n",
    "        # elif data.get(\"tool\") == \"HEPSYCODE\":\n",
    "            # Here you can choose what \"HEPSYCODE\" should map to.\n",
    "            # For example, if HEPSYCODE is related to vectorstore, then:\n",
    "        #    return RouteQuery(datasource=\"vectorstore\")\n",
    "        else:\n",
    "            raise ValueError(\"Output does not contain a valid routing decision.\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to parse router output: {e}\")\n",
    "\n",
    "# Starting node: route question decides between web_search and vectorstore (retrieve)\n",
    "def route_question(state: GraphState) -> str:\n",
    "    # If the flag is present, skip the routing and return a special key (\"skip\")\n",
    "    if state.get(\"skip_router\", False):\n",
    "        print(\"Skipping routing; moving directly to cache_context.\")\n",
    "        return \"skip\"\n",
    "        \n",
    "    print(\"--- ROUTE QUESTION ---\")\n",
    "    question_val = state[\"question\"]\n",
    "    print(question_val)\n",
    "    print(topics_str)\n",
    "    source = question_router.invoke({\"question\": question_val, \"topics_str\": topics_str})\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed = custom_parse_router_output(source)\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing router output:\", e)\n",
    "        # Fallback to a default value\n",
    "        parsed = RouteQuery(datasource=\"web_search\")\n",
    "    \"\"\"\n",
    "        \n",
    "    # Normalize the datasource value.\n",
    "    datasource = source.datasource.lower().strip()\n",
    "    if datasource == \"vectorstore\":\n",
    "        print(\"--- Routing to vectorstore ---\")\n",
    "        state[\"branch\"] = \"retrieve\"\n",
    "        return \"vectorstore\"\n",
    "    elif datasource == \"web_search\":\n",
    "        print(\"--- Routing to web search ---\")\n",
    "        state[\"branch\"] = \"web_search\"\n",
    "        return \"web_search\"\n",
    "    state[\"branch\"] = \"retrieve\"\n",
    "    return \"vectorstore\"\n",
    "\n",
    "# Add an edge from the START node to the new \"generate_query\" node\n",
    "workflow.add_edge(START, \"generate_query\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_query\",\n",
    "    route_question,\n",
    "    {\n",
    "        \"skip\": \"cache_context\", # If the flag is active, go directly to cache_context_node\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"vectorstore\": \"retrieve\",  # Key now matches the returned normalized value\n",
    "    },\n",
    ")\n",
    "\n",
    "# For the web search branch, send directly to generate.\n",
    "workflow.add_edge(\"web_search\", \"generate_web\")\n",
    "\n",
    "# For the retrieve branch, first go to grade_documents.\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# After grading, decide whether to generate or transform.\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    lambda state: decide_to_generate(state),\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_edge(\"transform_query_web\", \"web_search\")\n",
    "\n",
    "# After generate/generate_web, grade the generation.\n",
    "# If the generation is \"useful\", route to the caching node.\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        # \"not supported\": \"generate\",\n",
    "        \"useful\": \"evaluate_rag_output\",\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"evaluate_rag_output\", \"evaluate_bert_score\")\n",
    "workflow.add_edge(\"evaluate_bert_score\", \"cache_context\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_web\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        # \"not supported\": \"generate_web\",\n",
    "        \"useful\": \"evaluate_web_search_output\",\n",
    "        \"not useful\": \"transform_query_web\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"evaluate_web_search_output\", \"evaluate_web_bert_score\")\n",
    "workflow.add_edge(\"evaluate_web_bert_score\", \"cache_context\")\n",
    "\n",
    "# After caching, flow to trace generation.\n",
    "workflow.add_edge(\"cache_context\", END)\n",
    "\n",
    "#workflow.add_edge(\"cache_context\", \"trace_generation\")\n",
    "# workflow.add_edge(\"trace_generation\", END)  # End the workflow after trace generation\n",
    "\n",
    "# Compile the workflow graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# Optionally visualize the graph (requires additional dependencies)\n",
    "\"\"\"\n",
    "try:\n",
    "    from IPython.display import display, Markdown, Image\n",
    "    # Retrieve the graph and set its configuration\n",
    "    graph = app.get_graph()\n",
    "    graph.mermaid_config = {\"graph_direction\": \"TD\"}\n",
    "\n",
    "    # Generate the PNG image bytes from the graph\n",
    "    png_bytes = graph.draw_mermaid_png()\n",
    "\n",
    "    # Save the image to disk as 'graph.png'\n",
    "    with open(\"graph.png\", \"wb\") as f:\n",
    "        f.write(png_bytes)\n",
    "    print(\"The graph has been saved as 'graph.png'.\")\n",
    "    \n",
    "    display(Markdown(\"### LangGraph Visualization ###\"))\n",
    "    display(Image(graph.draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(\"Graph rendering failed:\", e)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d92ba068-b432-435b-8b6d-9d5b75138879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Profiling] wrapper took 4.7826 seconds\n",
      "Generated query from metamodel: Metamodel Analysis Query:\n",
      "Package Name: hepsy\n",
      "Namespace URI: org.univaq.hepsy\n",
      "Classifiers: None\n",
      "Based solely on the metamodel information provided above, generate a context for a tool based on this metamodel. The context should include the tool's name, which must be directly derived from the package name.\n",
      "[Profiling] wrapper took 9.6021 seconds\n",
      "--- ROUTE QUESTION ---\n",
      "Metamodel Analysis Query:\n",
      "Package Name: hepsy\n",
      "Namespace URI: org.univaq.hepsy\n",
      "Classifiers: None\n",
      "Based solely on the metamodel information provided above, generate a context for a tool based on this metamodel. The context should include the tool's name, which must be directly derived from the package name.\n",
      "CAEX/AutomationML, BPMN Designer, HEPSYCODE\n",
      "--- Routing to vectorstore ---\n",
      "--- RETRIEVE ---\n",
      "[Profiling] wrapper took 4.8973 seconds\n",
      "--- GRADE DOCUMENTS ---\n",
      "--- Document is not relevant ---\n",
      "Rate limit exceeded, waiting for 10 seconds before retrying...\n",
      "--- Document is relevant ---\n",
      "The retrieval_grader did not return a valid result for this document, skipping it.\n",
      "The retrieval_grader did not return a valid result for this document, skipping it.\n",
      "[Profiling] wrapper took 29.6590 seconds\n",
      "--- DECIDE TO GENERATE ---\n",
      "--- Relevant documents found, generating answer ---\n",
      "--- GENERATE (RAG) ---\n",
      "--- GRADE GENERATION ---\n",
      "--- Generation is grounded in documents ---\n",
      "[Profiling] wrapper took 6.1251 seconds\n",
      "--- EVALUATE RAG OUTPUT METRICS ---\n",
      "Evaluation metrics: {'faithfulness': 0.9, 'answer_relevance': 0.9, 'context_precision': 0.8, 'context_accuracy': 0.85, 'context_recall': 0.8, 'context_f1': 0.8}\n",
      "[Profiling] wrapper took 8.6866 seconds\n",
      "--- EVALUATE BERT SCORE ---\n",
      "RAG context:  ## Electronic Design Automation for Embedded Systems Using HEPSYCODE\n",
      "\n",
      "### Overview\n",
      "\n",
      "Electronic Design Automation (EDA) is a critical field in the design and development of electronic systems, including embedded systems. EDA tools facilitate the design of complex integrated circuits (ICs) and printed circuit boards (PCBs) by automating various aspects of the design process. HEPSYCODE is a prototypal tool specifically designed for the HW/SW co-design of heterogeneous parallel dedicated systems, focusing on embedded applications.\n",
      "\n",
      "### HEPSYCODE: A Comprehensive Framework\n",
      "\n",
      "#### Introduction to HEPSYCODE\n",
      "\n",
      "HEPSYCODE (HW/SW CO-DEsign of HEterogeneous Parallel dedicated SYstems) is a tool developed to streamline the design process of embedded applications. It employs a System-Level methodology for HW/SW Co-Design, guiding designers from an Electronic System-Level (ESL) behavioral model to the final HW/SW implementation. The framework considers specific hardware technologies, scheduling policies, and Inter-Process Communication (IPC) mechanisms.\n",
      "\n",
      "#### Key Components\n",
      "\n",
      "1. **HEPSY Modeling Language (HML)**:\n",
      "   - Based on the Communicating Sequential Processes (CSP) Model of Computation (MoC).\n",
      "   - Models system behavior as a network of processes communicating through unidirectional synchronous channels.\n",
      "   - Specifies the System Behavior Model (SBM), Non Functional Constraints (NFC), and Reference Inputs (RI) for simulation-based activities.\n",
      "\n",
      "2. **Design Space Exploration (DSE)**:\n",
      "   - A system-level DSE approach suggests HW/SW partitioning of the application specification.\n",
      "   - Maps partitioned entities onto an automatically defined heterogeneous multi-processor architecture.\n",
      "\n",
      "3. **Technologies and Tools**:\n",
      "   - Utilizes Eclipse MDE technologies, SystemC custom simulator implementation, and an evolutionary genetic algorithm for partitioning activities.\n",
      "   - Integrates reference libraries, scripts, makefiles, Eclipse plugins, XML data exchange files, SystemC files, and HW/SW Partitioning And Mapping (PAM) tools.\n",
      "\n",
      "#### Installation and Setup\n",
      "\n",
      "To set up HEPSYCODE, follow these steps:\n",
      "\n",
      "1. **Download Eclipse Modelling Tool**:\n",
      "   - Available at [Eclipse Downloads](https://www.eclipse.org/downloads/eclipse-packages/).\n",
      "\n",
      "2. **Clone HEPSYCODE Repository**:\n",
      "   - GitHub repository: [HEPSYCODE-AIDOaRt](https://github.com/HEPSYCODE/HEPSYCODE-AIDOaRt).\n",
      "\n",
      "3. **Import HEPSYCODE Projects**:\n",
      "   - Import the projects present in the folder into Eclipse.\n",
      "   - Run a separate Eclipse application to run and debug the HEPSYCODE plug-in.\n",
      "\n",
      "4. **Install SystemC Library**:\n",
      "   - Recommended version: 2.3.3.\n",
      "   - Update the `.bashrc` file with the SystemC path:\n",
      "     ```bash\n",
      "     export SYSTEMCPATHLIB=/usr/local/systemc-2.3.3/lib-linux64\n",
      "     export SYSTEMCPATHINCLUDE=/usr/local/systemc-2.3.3/include\n",
      "     ```\n",
      "   - Alternatively, run the `settings.sh` script.\n",
      "\n",
      "#### System Requirements\n",
      "\n",
      "- **Operating System**: Linux OS or Windows.\n",
      "- **Libraries**: SystemC Libraries version 2.3.3.\n",
      "- **Eclipse Plugins**: Eclipse Sirius, Eclipse Xtext.\n",
      "\n",
      "### Embedded Systems and EDA\n",
      "\n",
      "#### Embedded Systems\n",
      "\n",
      "Embedded systems are specialized computer systems designed to perform specific tasks within a larger mechanical or electronic system. They often have real-time computing constraints and are used in various applications, from consumer electronics to industrial automation.\n",
      "\n",
      "#### EDA for Embedded Systems\n",
      "\n",
      "EDA tools are essential for designing embedded systems due to their complexity. Key aspects of EDA for embedded systems include:\n",
      "\n",
      "1. **Design Flow**:\n",
      "   - Involves front-end tools producing standardized design descriptions.\n",
      "   - Back-end tools perform logic synthesis and simulation.\n",
      "\n",
      "2. **Tools and Technologies**:\n",
      "   - Early EDA tools were academic, such as the Berkeley VLSI Tools Tarball.\n",
      "   - Modern tools include Verilog and VHDL for hardware description.\n",
      "   - EDA companies like Mentor Graphics and Cadence Design Systems provide comprehensive tool suites.\n",
      "\n",
      "3. **Challenges**:\n",
      "   - Embedded systems often require specialized knowledge for analog design.\n",
      "   - Real-time performance constraints and limited hardware resources.\n",
      "\n",
      "### AutomationML and Embedded Systems\n",
      "\n",
      "AutomationML is a neutral data format based on XML for the storage and exchange of plant engineering information. It interconnects the heterogeneous tool landscape of modern engineering tools, making it relevant for embedded systems design.\n",
      "\n",
      "#### Key Features\n",
      "\n",
      "1. **Object-Oriented Description**:\n",
      "   - Describes real plant components as objects encapsulating different aspects.\n",
      "   - Supports topology, geometry, kinematics, and logic.\n",
      "\n",
      "2. **Integration of Standards**:\n",
      "   - Incorporates various standards through strongly typed links across formats.\n",
      "   - Designed for future extensions to integrate additional formats.\n",
      "\n",
      "3. **Open Standard**:\n",
      "   - Available free of charge.\n",
      "   - Initiated by Daimler and standardized by a consortium of companies and institutions.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "HEPSYCODE, combined with EDA tools and frameworks like AutomationML, provides a robust solution for the design and development of embedded systems. By leveraging System-Level methodologies and advanced simulation tools, HEPSYCODE streamlines the HW/SW co-design process, ensuring efficient and reliable embedded system implementations.\n",
      "\n",
      "### References\n",
      "\n",
      "1. [HEPSYCODE-AIDOaRt GitHub Repository](https://github.com/HEPSYCODE/HEPSYCODE-AIDOaRt)\n",
      "2. [HEPSYCODE Official Website](http://www.hepsycode.com/)\n",
      "3. [Electronic Design Automation - Wikipedia](https://en.wikipedia.org/wiki/Electronic_design_automation)\n",
      "4. [Embedded System - Wikipedia](https://en.wikipedia.org/wiki/Embedded_system)\n",
      "5. [AutomationML - Wikipedia](https://en.wikipedia.org/wiki/AutomationML)\n",
      "\n",
      "This refined context provides a comprehensive overview of Electronic Design Automation for embedded systems using HEPSYCODE, integrating relevant details from multiple sources to ensure a cohesive and informative explanation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bd7497e8eb4cc6ac3d37ddee435293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eeda86b1f304abc92304cb7443c270f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.29 seconds, 0.44 sentences/sec\n",
      "RAG BERTScore metrics: {'precision': 0.8117960095405579, 'recall': 0.8121114373207092, 'f1': 0.8119536638259888, 'cosine_similarity': 0.2620988240136142, 'levenshtein_distance': 0.19147861232738295}\n",
      "[Profiling] wrapper took 8.9688 seconds\n",
      "Generating refined context in LangGraph cache node...\n",
      "Refined context cached to file from LangGraph node.\n",
      "[Profiling] wrapper took 51.1670 seconds\n",
      "Profiling data for test-rag.hepsy saved to D2-HEPSYCODE-RAG-EVAL-NEW/faiss/XES-MORGAN-RAG-LLM-mistral-large-latest-0.0/JSON\\profiling_test-rag.hepsy.csv\n",
      "CodeCarbon metrics for test-rag.hepsy saved to D2-HEPSYCODE-RAG-EVAL-NEW/faiss/XES-MORGAN-RAG-LLM-mistral-large-latest-0.0/JSON\\codecarbon_test-rag.hepsy.csv\n",
      "skip_router: False\n",
      "evaluation_metrics: {'faithfulness': 0.9, 'answer_relevance': 0.9, 'context_precision': 0.8, 'context_accuracy': 0.85, 'context_recall': 0.8, 'context_f1': 0.8}\n",
      "bert_score_metrics: {'precision': 0.8117960095405579, 'recall': 0.8121114373207092, 'f1': 0.8119536638259888, 'cosine_similarity': 0.2620988240136142, 'levenshtein_distance': 0.19147861232738295}\n",
      "evaluation_metrics: None\n",
      "Evaluation results for test-rag.hepsy saved to D2-HEPSYCODE-RAG-EVAL-NEW/faiss/XES-MORGAN-RAG-LLM-mistral-large-latest-0.0/JSON\\evaluation_test-rag.hepsy.csv\n",
      "Summary profiling data saved to D2-HEPSYCODE-RAG-EVAL-NEW/faiss/XES-MORGAN-RAG-LLM-mistral-large-latest-0.0/JSON\\profiling_summary.csv\n",
      "Global CodeCarbon summary saved to D2-HEPSYCODE-RAG-EVAL-NEW/faiss/XES-MORGAN-RAG-LLM-mistral-large-latest-0.0/JSON\\codecarbon_summary.csv\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#       TRACE GENERATION LOOP     #\n",
    "###################################\n",
    "\n",
    "# Global CodeCarbon tracker for the entire application\n",
    "global_cc_tracker = EmissionsTracker(\n",
    "    project_name=\"global_app\",\n",
    "    measure_power_secs=1,\n",
    "    output_dir=CODECARBON_FOLDER,\n",
    "    allow_multiple_runs=True\n",
    "    # api_call_interval=4,\n",
    "    # experiment_id=experiment_id,\n",
    "    # save_to_api=True\n",
    "    # log_to_api=True                     # Enable logging to the CodeCarbon online dashboard\n",
    "    # api_key=codecarbon_api_key,          # Provide your CodeCarbon API key here\n",
    "    # api_url=\"https://api.codecarbon.io\"   # (Optional) Specify the API endpoint if different from the default\n",
    ")\n",
    "global_cc_tracker.start()\n",
    "\n",
    "# Reset the overall summary for CodeCarbon per file\n",
    "cc_global_summary = []\n",
    "\n",
    "# List to collect summary records for each file (for final summary CSV)\n",
    "summary_records = []\n",
    "\n",
    "app_start_time = time.time()\n",
    "\n",
    "# For each file, add file name and the query to the state.\n",
    "# The cache_context node in the workflow will ensure the refined context is present.\n",
    "# input_files = [file_name for file_name in os.listdir(base_model_path) if file_name.endswith(\".hepsy\")]\n",
    "\n",
    "file_name = \"test-rag.hepsy\"\n",
    "\n",
    "# for file_name in input_files:\n",
    "# Record start time for this file\n",
    "file_start = time.time()\n",
    "\n",
    "# Record the starting index of the global profiling_records list\n",
    "start_index = len(profiling_records)\n",
    "\n",
    "# Start a file-level CodeCarbon tracker\n",
    "file_cc_tracker = EmissionsTracker(\n",
    "    project_name=\"global_file_\" + file_name,\n",
    "    measure_power_secs=1,\n",
    "    output_dir=CODECARBON_FOLDER,\n",
    "    allow_multiple_runs=True,\n",
    "    api_call_interval=4\n",
    "    # experiment_id=experiment_id,\n",
    "    # save_to_api=True\n",
    "    # log_to_api=True                     # Enable logging to the CodeCarbon online dashboard for each file\n",
    "    # api_key=codecarbon_api_key,          # Provide your CodeCarbon API key here\n",
    "    # api_url=\"https://api.codecarbon.io\"   # (Optional) Specify the API endpoint if different from the default\n",
    ")\n",
    "file_cc_tracker.start()\n",
    "\n",
    "# Reset the per-node CodeCarbon metrics for this file\n",
    "cc_metrics_for_file = []\n",
    "\n",
    "state = GraphState()\n",
    "state[\"file_name\"] = file_name         # Provide file name for trace generation\n",
    "# state[\"question\"] = question           # The query generated from the metamodel\n",
    "# Run the workflow using stream() and take the final output state\n",
    "result_state = list(app.stream(state, config={\"recursion_limit\": 25}))[-1]\n",
    "\n",
    "# Record end time for this file and calculate overall time\n",
    "file_end = time.time()\n",
    "overall_time = file_end - file_start\n",
    "\n",
    "# Stop the file-level CodeCarbon tracker and get global metrics for the file\n",
    "file_emissions = file_cc_tracker.stop()\n",
    "# Try to get detailed metrics if available\n",
    "if hasattr(file_cc_tracker, \"_final_emissions_data\"):\n",
    "    file_metrics = file_cc_tracker._final_emissions_data\n",
    "else:\n",
    "    file_metrics = {\"total_emissions\": file_emissions}\n",
    "\n",
    "# Extract profiling records corresponding to this file\n",
    "file_records = profiling_records[start_index:].copy()\n",
    "# Append an additional record for the overall file execution time\n",
    "file_records.append({\"node\": f\"FILE_{file_name}\", \"execution_time\": overall_time})\n",
    "\n",
    "# Save the profiling data for this file in a dedicated CSV file if it doesn't already exist\n",
    "csv_file_path = os.path.join(PROFILING_FOLDER, f\"profiling_{file_name}.csv\")\n",
    "if not os.path.exists(csv_file_path):\n",
    "    with open(csv_file_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"node\", \"execution_time\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for record in file_records:\n",
    "            writer.writerow(record)\n",
    "    print(f\"Profiling data for {file_name} saved to {csv_file_path}\")\n",
    "else:\n",
    "    print(f\"Profiling file {csv_file_path} already exists. Skipping save.\")\n",
    "\n",
    "# Add a summary record for this file\n",
    "summary_records.append({\"file_name\": file_name, \"execution_time\": overall_time})\n",
    "\n",
    "############ CODE CARBON ##############\n",
    "# Save per-node CodeCarbon metrics along with file-level metrics into a dedicated CSV file,\n",
    "# with file name starting with \"codecarbon_\"\n",
    "cc_csv_file = os.path.join(CODECARBON_FOLDER, f\"codecarbon_{file_name}.csv\")\n",
    "if not os.path.exists(cc_csv_file):\n",
    "    # Prepare a list of rows: one row per node metric, plus one row for overall file metrics.\n",
    "    # We merge the per-node metrics (from cc_metrics_for_file) into a list.\n",
    "    # Note: Each metric row is a dictionary. We also add a row for the file global metrics.\n",
    "    rows = []\n",
    "    for record in cc_metrics_for_file:\n",
    "        # record already contains \"node\" and various CodeCarbon metrics\n",
    "        rows.append(record)\n",
    "    # Append a row for overall file CodeCarbon metrics:\n",
    "    overall_record = {\"node\": f\"FILE_{file_name}\"}\n",
    "    overall_record.update(file_metrics)\n",
    "    rows.append(overall_record)\n",
    "    \n",
    "    # Determine all possible keys across all rows\n",
    "    all_keys = set()\n",
    "    for r in rows:\n",
    "        all_keys.update(r.keys())\n",
    "    all_keys = list(all_keys)\n",
    "    \n",
    "    with open(cc_csv_file, mode=\"w\", newline=\"\") as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=all_keys)\n",
    "        writer.writeheader()\n",
    "        for r in rows:\n",
    "            writer.writerow(r)\n",
    "    print(f\"CodeCarbon metrics for {file_name} saved to {cc_csv_file}\")\n",
    "else:\n",
    "    print(f\"CodeCarbon file {cc_csv_file} already exists. Skipping save.\")\n",
    "\n",
    "# Append summary record for this file (global CodeCarbon metrics)\n",
    "cc_global_summary.append({\"file_name\": file_name, **file_metrics})\n",
    "\n",
    "############## RAG EVALUATION ################\n",
    "\n",
    "skip_router_value = result_state[\"cache_context\"][\"skip_router\"]\n",
    "print(\"skip_router:\", skip_router_value)\n",
    "\n",
    "evaluation_metrics = result_state[\"cache_context\"].get(\"evaluation_metrics\")\n",
    "print(\"evaluation_metrics:\", evaluation_metrics)\n",
    "\n",
    "bert_score_metrics = result_state[\"cache_context\"].get(\"bert_score\")\n",
    "print(\"bert_score_metrics:\", bert_score_metrics)\n",
    "\n",
    "web_bert_score_metrics = result_state[\"cache_context\"].get(\"web_bert_score\")\n",
    "print(\"evaluation_metrics:\", web_bert_score_metrics)\n",
    "\n",
    "if not skip_router_value:\n",
    "    # Save evaluation results to CSV for this file (if evaluation metrics exist)\n",
    "    evaluation_data = {\"file_name\": file_name}\n",
    "    \n",
    "    # Use get() with a default empty dict to ensure we update with available metrics\n",
    "    evaluation_data.update(result_state[\"cache_context\"].get(\"evaluation_metrics\", {}))\n",
    "    evaluation_data.update(result_state[\"cache_context\"].get(\"bert_score\", {}))\n",
    "    evaluation_data.update(result_state[\"cache_context\"].get(\"web_bert_score\", {}))\n",
    "    \n",
    "    eval_csv_file = os.path.join(EVALUATION_FOLDER, f\"evaluation_{file_name}.csv\")\n",
    "    with open(eval_csv_file, \"w\", newline=\"\") as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=evaluation_data.keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerow(evaluation_data)\n",
    "    print(f\"Evaluation results for {file_name} saved to {eval_csv_file}\")\n",
    "\n",
    "# print(f\"Trace generation result for {file_name}: {result_state.get('trace_status', 'unknown')}\")\n",
    "\n",
    "# Record end time of the entire application and calculate total time\n",
    "app_end_time = time.time()\n",
    "total_app_time = app_end_time - app_start_time\n",
    "summary_records.append({\"file_name\": \"TOTAL_APP\", \"execution_time\": total_app_time})\n",
    "\n",
    "global_summary = global_cc_tracker.stop()\n",
    "if hasattr(global_cc_tracker, \"_final_emissions_data\"):\n",
    "    global_metrics = global_cc_tracker._final_emissions_data\n",
    "else:\n",
    "    global_metrics = {\"total_emissions\": global_summary}\n",
    "cc_global_summary.append({\"file_name\": \"TOTAL_APP\", **global_metrics})\n",
    "# print(\"END TRACE GENERATION PROCESS!!!\")\n",
    "\n",
    "# Save the final summary CSV with overall times per file if it doesn't already exist\n",
    "final_csv_file = os.path.join(PROFILING_FOLDER, \"profiling_summary.csv\")\n",
    "if not os.path.exists(final_csv_file):\n",
    "    with open(final_csv_file, mode=\"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"file_name\", \"execution_time\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for record in summary_records:\n",
    "            writer.writerow(record)\n",
    "    print(f\"Summary profiling data saved to {final_csv_file}\")\n",
    "else:\n",
    "    print(f\"Summary profiling CSV {final_csv_file} already exists. Skipping save.\")\n",
    "\n",
    "# Save the global CodeCarbon summary into a CSV file\n",
    "global_csv_file = os.path.join(CODECARBON_FOLDER, \"codecarbon_summary.csv\")\n",
    "if not os.path.exists(global_csv_file):\n",
    "    fieldnames = set()\n",
    "    for record in cc_global_summary:\n",
    "        fieldnames.update(record.keys())\n",
    "    fieldnames = list(fieldnames)\n",
    "    with open(global_csv_file, mode=\"w\", newline=\"\") as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for record in cc_global_summary:\n",
    "            writer.writerow(record)\n",
    "    print(f\"Global CodeCarbon summary saved to {global_csv_file}\")\n",
    "else:\n",
    "    print(f\"Global CodeCarbon summary file {global_csv_file} already exists. Skipping save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a6ea7-3fe4-40db-aba6-c35d52c8eba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
